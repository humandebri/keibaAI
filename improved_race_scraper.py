#!/usr/bin/env python3
"""
netkeiba.com æ”¹è‰¯ç‰ˆãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
ã‚ªãƒƒã‚ºã¨äººæ°—ã‚’ç¢ºå®Ÿã«å–å¾—ã™ã‚‹ç‰¹åŒ–å‹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import time
import random
from typing import Dict, List, Optional
import json

# Seleniumã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.webdriver.chrome.options import Options
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False


class ImprovedRaceScraper:
    """netkeiba.com ã‹ã‚‰ã‚ªãƒƒã‚ºã¨äººæ°—ã‚’ç¢ºå®Ÿã«å–å¾—ã™ã‚‹æ”¹è‰¯ç‰ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    PLACE_DICT = {
        "01": "æœ­å¹Œ", "02": "å‡½é¤¨", "03": "ç¦å³¶", "04": "æ–°æ½Ÿ", "05": "æ±äº¬",
        "06": "ä¸­å±±", "07": "ä¸­äº¬", "08": "äº¬éƒ½", "09": "é˜ªç¥", "10": "å°å€‰"
    }
    
    USER_AGENTS = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/91.0.864.67",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36",
        "Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1"
    ]
    
    def __init__(self):
        self.session = requests.Session()
        self._setup_session()
        
    def _setup_session(self):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        user_agent = random.choice(self.USER_AGENTS)
        self.session.headers.update({
            'User-Agent': user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
    
    def scrape_race_data(self, race_id: str, retry_count: int = 3) -> pd.DataFrame:
        """ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆè¤‡æ•°æ‰‹æ³•ã§ã‚ªãƒƒã‚ºãƒ»äººæ°—ã‚’ç¢ºå®Ÿã«å–å¾—ï¼‰"""
        print(f"ğŸ‡ ãƒ¬ãƒ¼ã‚¹æƒ…å ±å–å¾—ä¸­: {race_id}")
        
        # æ‰‹æ³•1: ã‚ªãƒƒã‚ºAPIçµŒç”±ã§ã®å–å¾—ã‚’è©¦ã™
        odds_data = self._try_odds_api(race_id)
        
        # æ‰‹æ³•2: å‹•çš„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆSeleniumï¼‰
        dynamic_data = None
        if SELENIUM_AVAILABLE:
            dynamic_data = self._scrape_with_selenium(race_id)
        
        # æ‰‹æ³•3: é™çš„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        static_data = self._scrape_static_improved(race_id)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã—ã¦æœ€è‰¯ã®çµæœã‚’é¸æŠ
        final_data = self._merge_data_sources(odds_data, dynamic_data, static_data, race_id)
        
        if final_data.empty:
            print("âŒ å…¨ã¦ã®æ‰‹æ³•ã§ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—")
            return pd.DataFrame()
        
        print(f"âœ… {len(final_data)}é ­ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—")
        return final_data
    
    def _try_odds_api(self, race_id: str) -> Optional[pd.DataFrame]:
        """ã‚ªãƒƒã‚ºAPIã¾ãŸã¯AjaxçµŒç”±ã§ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’è©¦è¡Œ"""
        try:
            # netkeiba.comã®ã‚ªãƒƒã‚ºå–å¾—ç”¨ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’è©¦ã™
            ajax_urls = [
                f"https://race.netkeiba.com/api/api_get_jockey_result.html?race_id={race_id}",
                f"https://race.netkeiba.com/odds/index.html?race_id={race_id}",
                f"https://race.netkeiba.com/race/oikiri.html?race_id={race_id}",
            ]
            
            for url in ajax_urls:
                try:
                    response = self.session.get(url, timeout=10)
                    if response.status_code == 200:
                        # JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å ´åˆ
                        if 'application/json' in response.headers.get('content-type', ''):
                            data = response.json()
                            parsed_data = self._parse_json_response(data, race_id)
                            if parsed_data is not None:
                                print("âœ“ APIçµŒç”±ã§ã‚ªãƒƒã‚ºãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ")
                                return parsed_data
                        
                        # HTMLãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å ´åˆ
                        soup = BeautifulSoup(response.content, 'html.parser')
                        parsed_data = self._parse_odds_page(soup, race_id)
                        if parsed_data is not None:
                            print("âœ“ ã‚ªãƒƒã‚ºãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ")
                            return parsed_data
                            
                except Exception as e:
                    continue
            
            return None
            
        except Exception as e:
            print(f"âš ï¸ APIå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _scrape_with_selenium(self, race_id: str) -> Optional[pd.DataFrame]:
        """Seleniumã‚’ä½¿ã£ãŸå‹•çš„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        url = f"https://race.netkeiba.com/race/shutuba.html?race_id={race_id}"
        
        # Chromeã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®šï¼ˆã‚ˆã‚Šå®‰å®šã—ãŸè¨­å®šï¼‰
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--disable-images')
        chrome_options.add_argument('--disable-javascript')  # JavaScriptã‚’ç„¡åŠ¹åŒ–ã—ã¦ãƒ­ãƒ¼ãƒ‰é«˜é€ŸåŒ–
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument(f'--user-agent={random.choice(self.USER_AGENTS)}')
        
        driver = None
        try:
            driver = webdriver.Chrome(options=chrome_options)
            driver.set_page_load_timeout(15)
            driver.implicitly_wait(5)
            
            # è¤‡æ•°ã®URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™
            urls_to_try = [
                f"https://race.netkeiba.com/race/shutuba.html?race_id={race_id}",
                f"https://race.netkeiba.com/odds/index.html?race_id={race_id}",
                f"https://nar.netkeiba.com/race/shutuba.html?race_id={race_id}",  # åœ°æ–¹ç«¶é¦¬ç”¨
            ]
            
            for url in urls_to_try:
                try:
                    driver.get(url)
                    
                    # ãƒ†ãƒ¼ãƒ–ãƒ«ã®èª­ã¿è¾¼ã¿å¾…æ©Ÿ
                    WebDriverWait(driver, 10).until(
                        EC.any_of(
                            EC.presence_of_element_located((By.CLASS_NAME, "Shutuba_Table")),
                            EC.presence_of_element_located((By.CLASS_NAME, "race_table_01")),
                            EC.presence_of_element_located((By.ID, "race_table"))
                        )
                    )
                    
                    # JavaScriptå®Ÿè¡Œå®Œäº†ã¾ã§å¾…æ©Ÿ
                    time.sleep(2)
                    
                    # ãƒšãƒ¼ã‚¸ã‚½ãƒ¼ã‚¹å–å¾—ã—ã¦è§£æ
                    soup = BeautifulSoup(driver.page_source, 'html.parser')
                    data = self._parse_shutuba_table_improved(soup, race_id)
                    
                    if data is not None and not data.empty:
                        print("âœ“ Seleniumå‹•çš„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æˆåŠŸ")
                        return data
                        
                except Exception as url_error:
                    continue
            
            return None
            
        except Exception as e:
            print(f"âš ï¸ Seleniumã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
            return None
            
        finally:
            if driver:
                try:
                    driver.quit()
                except:
                    pass
    
    def _scrape_static_improved(self, race_id: str) -> Optional[pd.DataFrame]:
        """æ”¹è‰¯ç‰ˆé™çš„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        urls_to_try = [
            f"https://race.netkeiba.com/race/shutuba.html?race_id={race_id}",
            f"https://race.netkeiba.com/odds/index.html?race_id={race_id}",
            f"https://nar.netkeiba.com/race/shutuba.html?race_id={race_id}",  # åœ°æ–¹ç«¶é¦¬ç”¨
        ]
        
        for url in urls_to_try:
            try:
                # ãƒ©ãƒ³ãƒ€ãƒ ãªå¾…æ©Ÿæ™‚é–“
                time.sleep(random.uniform(0.5, 2.0))
                
                response = self.session.get(url, timeout=15)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                data = self._parse_shutuba_table_improved(soup, race_id)
                
                if data is not None and not data.empty:
                    print("âœ“ é™çš„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æˆåŠŸ")
                    return data
                    
            except Exception as e:
                continue
        
        print("âš ï¸ é™çš„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—")
        return None
    
    def _parse_shutuba_table_improved(self, soup: BeautifulSoup, race_id: str) -> Optional[pd.DataFrame]:
        """æ”¹è‰¯ç‰ˆShutuba_Tableè§£æ"""
        
        # è¤‡æ•°ã®ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™
        table_selectors = [
            'table.Shutuba_Table',
            'table.race_table_01',
            'table[summary*="å‡ºé¦¬è¡¨"]',
            'table[id*="race"]',
            'div.race_table_wrapper table',
        ]
        
        table = None
        for selector in table_selectors:
            table = soup.select_one(selector)
            if table:
                print(f"âœ“ ãƒ†ãƒ¼ãƒ–ãƒ«ç™ºè¦‹: {selector}")
                break
        
        if not table:
            print("âŒ å‡ºé¦¬è¡¨ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
        
        horses_data = []
        rows = table.find_all('tr')
        
        print(f"ãƒ†ãƒ¼ãƒ–ãƒ«è¡Œæ•°: {len(rows)}")
        
        for row_idx, row in enumerate(rows):
            cells = row.find_all(['td', 'th'])
            if len(cells) < 8:  # æœ€ä½é™å¿…è¦ãªåˆ—æ•°
                continue
            
            # é¦¬ç•ªãƒã‚§ãƒƒã‚¯
            first_cell_text = cells[0].get_text(strip=True)
            if not first_cell_text.isdigit():
                continue
            
            horse_num = int(first_cell_text)
            if not (1 <= horse_num <= 18):
                continue
            
            # ã‚»ãƒ«å†…å®¹ã‚’ãƒ‡ãƒãƒƒã‚°è¡¨ç¤º
            print(f"\n--- é¦¬{horse_num}ç•ªãƒ‡ãƒ¼ã‚¿è§£æ ---")
            for i, cell in enumerate(cells[:12]):
                cell_text = cell.get_text(strip=True)
                cell_classes = cell.get('class', [])
                print(f"ã‚»ãƒ«{i}: '{cell_text}' class={cell_classes}")
            
            # é¦¬ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            horse_data = self._extract_horse_data_improved(cells, race_id, horse_num)
            if horse_data:
                horses_data.append(horse_data)
        
        if not horses_data:
            print("âŒ é¦¬ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡ºã«å¤±æ•—")
            return None
        
        return pd.DataFrame(horses_data)
    
    def _extract_horse_data_improved(self, cells: List, race_id: str, horse_num: int) -> Optional[Dict]:
        """æ”¹è‰¯ç‰ˆé¦¬ãƒ‡ãƒ¼ã‚¿æŠ½å‡º"""
        try:
            # åŸºæœ¬æƒ…å ±ã®æŠ½å‡º
            data = {
                'race_id': race_id,
                'é¦¬ç•ª': horse_num,
                'æ ': self._extract_waku(cells),
                'é¦¬å': self._extract_horse_name(cells),
                'æ€§é½¢': self._extract_age_sex(cells),
                'é¨æ‰‹': self._extract_jockey(cells),
                'é¦¬èˆ': self._extract_trainer(cells),
                'æ–¤é‡': self._extract_weight_carried(cells),
                'é¦¬ä½“é‡': self._extract_horse_weight(cells),
                'ã‚ªãƒƒã‚º': self._extract_odds_improved(cells),
                'äººæ°—': self._extract_popularity_improved(cells)
            }
            
            # ãƒ‡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
            if data['é¦¬å'] == "ä¸æ˜" or data['é¨æ‰‹'] == "ä¸æ˜":
                print(f"âš ï¸ é¦¬{horse_num}ç•ª: åŸºæœ¬æƒ…å ±ä¸è¶³")
                return None
            
            # ã‚ªãƒƒã‚ºã¨äººæ°—ã®çŠ¶æ³ã‚’å ±å‘Š
            odds_status = "å–å¾—" if data['ã‚ªãƒƒã‚º'] is not None else "æœªè¨­å®š"
            pop_status = "å–å¾—" if data['äººæ°—'] is not None else "æœªè¨­å®š"
            
            print(f"âœ“ é¦¬{horse_num}ç•ª: {data['é¦¬å']} / {data['é¨æ‰‹']} / ã‚ªãƒƒã‚º{odds_status} / äººæ°—{pop_status}")
            
            return data
            
        except Exception as e:
            print(f"âŒ é¦¬{horse_num}ç•ªãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _extract_waku(self, cells: List) -> int:
        """æ ç•ªã‚’æŠ½å‡º"""
        # 1åˆ—ç›®ã¾ãŸã¯æ é–¢é€£ã‚¯ãƒ©ã‚¹ã‹ã‚‰æŠ½å‡º
        for i, cell in enumerate(cells[:3]):
            if 'Waku' in str(cell.get('class', [])):
                text = cell.get_text(strip=True)
                if text.isdigit() and 1 <= int(text) <= 8:
                    return int(text)
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é¦¬ç•ªã‹ã‚‰æ¨æ¸¬
        horse_num = int(cells[1].get_text(strip=True)) if len(cells) > 1 else 1
        return ((horse_num - 1) // 2) + 1
    
    def _extract_horse_name(self, cells: List) -> str:
        """é¦¬åã‚’æŠ½å‡º"""
        for cell in cells[2:6]:
            # HorseInfoã‚¯ãƒ©ã‚¹ã¾ãŸã¯é¦¬ã¸ã®ãƒªãƒ³ã‚¯
            if 'HorseInfo' in str(cell.get('class', [])):
                return cell.get_text(strip=True)
            
            horse_link = cell.find('a', href=lambda href: href and 'horse' in href)
            if horse_link:
                return horse_link.get_text(strip=True)
        
        return "ä¸æ˜"
    
    def _extract_age_sex(self, cells: List) -> str:
        """æ€§é½¢ã‚’æŠ½å‡º"""
        for cell in cells[3:7]:
            if 'Barei' in str(cell.get('class', [])):
                return cell.get_text(strip=True)
        
        return "ä¸æ˜"
    
    def _extract_jockey(self, cells: List) -> str:
        """é¨æ‰‹ã‚’æŠ½å‡º"""
        for cell in cells[4:8]:
            # Jockeyã‚¯ãƒ©ã‚¹ã¾ãŸã¯é¨æ‰‹ã¸ã®ãƒªãƒ³ã‚¯
            if 'Jockey' in str(cell.get('class', [])):
                jockey_link = cell.find('a')
                if jockey_link:
                    return jockey_link.get_text(strip=True)
                return cell.get_text(strip=True)
            
            jockey_link = cell.find('a', href=lambda href: href and 'jockey' in href)
            if jockey_link:
                return jockey_link.get_text(strip=True)
        
        return "ä¸æ˜"
    
    def _extract_trainer(self, cells: List) -> str:
        """èª¿æ•™å¸«ã‚’æŠ½å‡º"""
        for cell in cells[5:9]:
            if 'Trainer' in str(cell.get('class', [])):
                trainer_link = cell.find('a')
                if trainer_link:
                    return trainer_link.get_text(strip=True)
                return cell.get_text(strip=True)
        
        return "ä¸æ˜"
    
    def _extract_weight_carried(self, cells: List) -> float:
        """æ–¤é‡ã‚’æŠ½å‡º"""
        for cell in cells[4:8]:
            text = cell.get_text(strip=True)
            if re.match(r'^5[0-9]\.[05]$', text):  # 50.0-59.5ã®ç¯„å›²
                return float(text)
        
        return 57.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    def _extract_horse_weight(self, cells: List) -> str:
        """é¦¬ä½“é‡ã‚’æŠ½å‡º"""
        for cell in cells[6:10]:
            if 'Weight' in str(cell.get('class', [])):
                return cell.get_text(strip=True)
            
            text = cell.get_text(strip=True)
            if re.match(r'\d{3,4}\([+-]?\d+\)', text):  # ä½“é‡(å¢—æ¸›)ã®å½¢å¼
                return text
        
        return "ä¸æ˜"
    
    def _extract_odds_improved(self, cells: List) -> Optional[float]:
        """æ”¹è‰¯ç‰ˆã‚ªãƒƒã‚ºæŠ½å‡º"""
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: å‹•çš„ã«èª­ã¿è¾¼ã¾ã‚ŒãŸã‚ªãƒƒã‚ºspan
        for cell in cells:
            odds_spans = cell.find_all('span', id=lambda x: x and 'odds-' in x)
            for span in odds_spans:
                odds_text = span.get_text(strip=True)
                if odds_text and odds_text not in ['---.-', '**', '--']:
                    try:
                        return float(odds_text)
                    except:
                        continue
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ã‚ªãƒƒã‚ºå°‚ç”¨ã‚¯ãƒ©ã‚¹
        odds_classes = ['Odds', 'odds', 'Popular', 'Txt_R']
        for cell in cells:
            cell_classes = cell.get('class', [])
            if any(odds_class in str(cell_classes) for odds_class in odds_classes):
                odds_text = cell.get_text(strip=True)
                if (odds_text and odds_text not in ['---.-', '**', '--'] and
                    re.match(r'^\d+\.\d+$', odds_text)):
                    try:
                        odds_val = float(odds_text)
                        if 1.0 <= odds_val <= 999.0 and not (50.0 <= odds_val <= 60.0):
                            return odds_val
                    except:
                        continue
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: æ•°å€¤ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        for cell in cells[7:]:  # å¾ŒåŠã®ã‚»ãƒ«ã‚’é‡ç‚¹çš„ã«
            text = cell.get_text(strip=True)
            # ã‚ˆã‚Šå³å¯†ãªã‚ªãƒƒã‚ºãƒ‘ã‚¿ãƒ¼ãƒ³
            odds_patterns = [
                r'^(\d{1,3}\.\d{1})$',        # åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³: 12.3
                r'^(\d{1,2}\.\d{2})$',        # è©³ç´°ãƒ‘ã‚¿ãƒ¼ãƒ³: 12.34
                r'å˜å‹.*?(\d+\.\d+)',         # å˜å‹ã‚ªãƒƒã‚º
                r'ã‚ªãƒƒã‚º.*?(\d+\.\d+)',       # ã‚ªãƒƒã‚ºè¡¨è¨˜
            ]
            
            for pattern in odds_patterns:
                match = re.search(pattern, text)
                if match:
                    try:
                        odds_val = float(match.group(1))
                        # ã‚ˆã‚Šå³å¯†ãªç¯„å›²ãƒã‚§ãƒƒã‚¯
                        if (1.0 <= odds_val <= 999.0 and 
                            not (50.0 <= odds_val <= 60.0) and  # æ–¤é‡é™¤å¤–
                            not (2020 <= odds_val <= 2030)):    # å¹´å·é™¤å¤–
                            return odds_val
                    except:
                        continue
        
        return None
    
    def _extract_popularity_improved(self, cells: List) -> Optional[int]:
        """æ”¹è‰¯ç‰ˆäººæ°—æŠ½å‡º"""
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: Popular_Ninkiã‚¯ãƒ©ã‚¹
        for cell in cells:
            if 'Popular_Ninki' in str(cell.get('class', [])):
                pop_text = cell.get_text(strip=True)
                if pop_text and pop_text not in ['**', '--', '***']:
                    try:
                        pop_val = int(pop_text)
                        if 1 <= pop_val <= 18:
                            return pop_val
                    except:
                        continue
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: äººæ°—é–¢é€£ã‚¯ãƒ©ã‚¹
        popularity_classes = ['Popular', 'popular', 'ninki', 'Ninki', 'rank', 'Rank']
        for cell in cells:
            cell_classes = cell.get('class', [])
            if any(pop_class in str(cell_classes) for pop_class in popularity_classes):
                pop_text = cell.get_text(strip=True)
                if (pop_text and pop_text not in ['**', '--', '***'] and 
                    pop_text.isdigit()):
                    try:
                        pop_val = int(pop_text)
                        if 1 <= pop_val <= 18:
                            return pop_val
                    except:
                        continue
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: èƒŒæ™¯è‰²ã«ã‚ˆã‚‹äººæ°—åˆ¤å®š
        for cell in cells:
            cell_classes = cell.get('class', [])
            if 'BgYellow' in cell_classes or 'bg-yellow' in cell_classes:
                return 1  # 1äººæ°—
            elif 'BgBlue' in cell_classes or 'bg-blue' in cell_classes:
                return 2  # 2äººæ°—
            elif 'BgOrange' in cell_classes or 'bg-orange' in cell_classes:
                return 3  # 3äººæ°—
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³4: ãƒ†ã‚­ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        for cell in cells[8:]:  # å¾ŒåŠã®ã‚»ãƒ«ã‚’é‡ç‚¹çš„ã«
            text = cell.get_text(strip=True)
            
            popularity_patterns = [
                r'^(\d{1,2})$',              # å˜ç´”ãªæ•°å­—
                r'(\d{1,2})äººæ°—',            # ã€Œ5äººæ°—ã€å½¢å¼
                r'äººæ°—(\d{1,2})',            # ã€Œäººæ°—5ã€å½¢å¼
                r'(\d{1,2})ç•ªäººæ°—',          # ã€Œ5ç•ªäººæ°—ã€å½¢å¼
            ]
            
            for pattern in popularity_patterns:
                match = re.search(pattern, text)
                if match:
                    try:
                        pop_val = int(match.group(1))
                        if 1 <= pop_val <= 18:
                            return pop_val
                    except:
                        continue
        
        return None
    
    def _parse_json_response(self, data: Dict, race_id: str) -> Optional[pd.DataFrame]:
        """JSON APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®è§£æ"""
        # å®Ÿè£…ã¯å¿…è¦ã«å¿œã˜ã¦è¿½åŠ 
        return None
    
    def _parse_odds_page(self, soup: BeautifulSoup, race_id: str) -> Optional[pd.DataFrame]:
        """ã‚ªãƒƒã‚ºãƒšãƒ¼ã‚¸ã®è§£æ"""
        # å®Ÿè£…ã¯å¿…è¦ã«å¿œã˜ã¦è¿½åŠ 
        return None
    
    def _merge_data_sources(self, odds_data: Optional[pd.DataFrame], 
                           dynamic_data: Optional[pd.DataFrame], 
                           static_data: Optional[pd.DataFrame], 
                           race_id: str) -> pd.DataFrame:
        """è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’çµ±åˆ"""
        
        # æœ€ã‚‚å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ã‚’é¸æŠ
        candidates = [data for data in [dynamic_data, static_data, odds_data] if data is not None]
        
        if not candidates:
            return pd.DataFrame()
        
        # ã‚ªãƒƒã‚ºã¨äººæ°—ã®å–å¾—æ•°ã§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        best_data = None
        best_score = -1
        
        for data in candidates:
            score = 0
            if not data.empty:
                # ã‚ªãƒƒã‚ºå–å¾—æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                odds_count = data['ã‚ªãƒƒã‚º'].notna().sum()
                popularity_count = data['äººæ°—'].notna().sum()
                
                score = odds_count * 2 + popularity_count  # ã‚ªãƒƒã‚ºã‚’é‡è¦–
                
                print(f"ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è©•ä¾¡: ã‚ªãƒƒã‚º{odds_count}é ­, äººæ°—{popularity_count}é ­, ã‚¹ã‚³ã‚¢{score}")
                
                if score > best_score:
                    best_score = score
                    best_data = data
        
        return best_data if best_data is not None else pd.DataFrame()


def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ”¹è‰¯ç‰ˆnetkeiba.comã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼')
    parser.add_argument('race_id', type=str, help='ãƒ¬ãƒ¼ã‚¹ID (ä¾‹: 202406020311)')
    parser.add_argument('--output', type=str, default='improved_race_data.csv', help='å‡ºåŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«')
    
    args = parser.parse_args()
    
    if not args.race_id.isdigit() or len(args.race_id) != 12:
        print("âŒ ãƒ¬ãƒ¼ã‚¹IDã¯12æ¡ã®æ•°å­—ã§å…¥åŠ›ã—ã¦ãã ã•ã„")
        return
    
    scraper = ImprovedRaceScraper()
    race_data = scraper.scrape_race_data(args.race_id)
    
    if race_data.empty:
        print("âŒ ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    # CSVä¿å­˜
    race_data.to_csv(args.output, index=False, encoding='utf-8')
    print(f"\nğŸ’¾ ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {args.output}")
    
    # çµæœè¡¨ç¤º
    print(f"\nğŸ“Š å–å¾—çµæœ: {len(race_data)}é ­")
    print("\nğŸ‡ å‡ºé¦¬è¡¨:")
    for _, horse in race_data.iterrows():
        odds_str = f"{horse['ã‚ªãƒƒã‚º']}å€" if pd.notna(horse['ã‚ªãƒƒã‚º']) else "æœªè¨­å®š"
        pop_str = f"{horse['äººæ°—']}äººæ°—" if pd.notna(horse['äººæ°—']) else "æœªè¨­å®š"
        print(f"  {horse['æ ']}æ {horse['é¦¬ç•ª']:2d}ç•ª {horse['é¦¬å']:15s} "
              f"{horse['é¨æ‰‹']:10s} {odds_str:8s} {pop_str}")


if __name__ == "__main__":
    main()