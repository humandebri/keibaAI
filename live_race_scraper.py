#!/usr/bin/env python3
"""
ãƒ©ã‚¤ãƒ–ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
æŒ‡å®šã•ã‚ŒãŸrace_idã‹ã‚‰ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import json
import argparse
import sys
import time
import random
from typing import Optional, Dict, List, Any


class LiveRaceScraper:
    """netkeiba.comã‹ã‚‰ãƒ©ã‚¤ãƒ–ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã‚’å–å¾—ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    USER_AGENTS = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/91.0.864.67"
    ]
    
    PLACE_DICT = {
        "01": "æœ­å¹Œ", "02": "å‡½é¤¨", "03": "ç¦å³¶", "04": "æ–°æ½Ÿ", "05": "æ±äº¬",
        "06": "ä¸­å±±", "07": "ä¸­äº¬", "08": "äº¬éƒ½", "09": "é˜ªç¥", "10": "å°å€‰"
    }
    
    def __init__(self, timeout: int = 10):
        """
        Args:
            timeout: ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç§’æ•°
        """
        self.timeout = timeout
    
    def fetch_race_data(self, race_id: str, max_retries: int = 3) -> Optional[bytes]:
        """æŒ‡å®šã•ã‚ŒãŸrace_idã‹ã‚‰ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        url = f"https://db.netkeiba.com/race/{race_id}"
        
        retries = 0
        while retries < max_retries:
            try:
                headers = {'User-Agent': random.choice(self.USER_AGENTS)}
                print(f"ğŸ“¡ ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­: {url}")
                
                r = requests.get(url, headers=headers, timeout=self.timeout)
                r.raise_for_status()
                
                print(f"âœ… ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ")
                return r.content
                
            except requests.exceptions.RequestException as e:
                retries += 1
                print(f"âŒ å–å¾—ã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {retries}/{max_retries}): {e}")
                
                if retries < max_retries:
                    wait_time = random.uniform(1, 3)
                    print(f"â³ {wait_time:.1f}ç§’å¾…æ©Ÿ...")
                    time.sleep(wait_time)
        
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—: {race_id}")
        return None
    
    def extract_odds_data(self, soup) -> Dict[str, float]:
        """ã‚ªãƒƒã‚ºãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        odds_data = {}
        
        try:
            # ã‚ªãƒƒã‚ºãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã™
            odds_table = soup.find("table", {"class": "race_table_01 nk_tb_common"})
            if not odds_table:
                return odds_data
            
            for row in odds_table.find_all("tr")[1:]:  # ãƒ˜ãƒƒãƒ€è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                cols = row.find_all("td")
                if len(cols) >= 13:
                    horse_num = cols[2].text.strip()
                    horse_name = cols[3].text.strip()
                    odds_text = cols[12].text.strip()
                    
                    # ã‚ªãƒƒã‚ºã‚’æ•°å€¤ã«å¤‰æ›
                    try:
                        odds = float(odds_text)
                        odds_data[f"{horse_num}ç•ª_{horse_name}"] = odds
                    except (ValueError, TypeError):
                        odds_data[f"{horse_num}ç•ª_{horse_name}"] = 0.0
            
        except Exception as e:
            print(f"âš ï¸ ã‚ªãƒƒã‚ºãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return odds_data
    
    def extract_race_info(self, soup, race_id: str) -> Dict[str, Any]:
        """ãƒ¬ãƒ¼ã‚¹åŸºæœ¬æƒ…å ±ã‚’æŠ½å‡º"""
        race_info = {
            'race_name': '',
            'date': '',
            'course': '',
            'distance': '',
            'class': '',
            'surface': '',
            'direction': '',
            'weather': '',
            'track_condition': '',
            'place': ''
        }
        
        try:
            # ãƒ¬ãƒ¼ã‚¹å
            title_elem = soup.find("h1")
            if title_elem:
                race_info['race_name'] = title_elem.text.strip()
            
            # è©³ç´°æƒ…å ±
            detail_elems = soup.find_all("p", class_="smalltxt")
            if detail_elems:
                detail_text = detail_elems[0].text.strip()
                parts = detail_text.split()
                if len(parts) >= 3:
                    race_info['date'] = parts[0]
                    race_info['course'] = parts[1]
                    race_info['class'] = parts[2]
            
            # ã‚³ãƒ¼ã‚¹æƒ…å ±ï¼ˆè·é›¢ã€èŠ/ãƒ€ãƒ¼ãƒˆã€å›ã‚Šï¼‰
            span_elems = soup.find_all("span")
            for span in span_elems:
                span_text = span.text.strip()
                if 'm' in span_text and ('èŠ' in span_text or 'ãƒ€' in span_text):
                    # è·é›¢æŠ½å‡º
                    try:
                        race_info['distance'] = ''.join(filter(str.isdigit, span_text.split('m')[0]))
                    except:
                        pass
                    
                    # èŠ/ãƒ€ãƒ¼ãƒˆ
                    if 'èŠ' in span_text:
                        race_info['surface'] = 'èŠ'
                    elif 'ãƒ€' in span_text:
                        race_info['surface'] = 'ãƒ€ãƒ¼ãƒˆ'
                    
                    # å›ã‚Š
                    if 'å³' in span_text:
                        race_info['direction'] = 'å³'
                    elif 'å·¦' in span_text:
                        race_info['direction'] = 'å·¦'
                    
                    # å¤©æ°—ã¨é¦¬å ´çŠ¶æ…‹
                    if 'æ™´' in span_text:
                        race_info['weather'] = 'æ™´'
                    elif 'æ›‡' in span_text:
                        race_info['weather'] = 'æ›‡'
                    elif 'é›¨' in span_text:
                        race_info['weather'] = 'é›¨'
                    
                    if 'è‰¯' in span_text:
                        race_info['track_condition'] = 'è‰¯'
                    elif 'ç¨é‡' in span_text:
                        race_info['track_condition'] = 'ç¨é‡'
                    elif 'é‡' in span_text:
                        race_info['track_condition'] = 'é‡'
                    elif 'ä¸è‰¯' in span_text:
                        race_info['track_condition'] = 'ä¸è‰¯'
            
            # é–‹å‚¬å ´æ‰€
            place_code = race_id[:6][4:6]
            race_info['place'] = self.PLACE_DICT.get(place_code, 'ä¸æ˜')
            
            # ãƒ¬ãƒ¼ã‚¹IDã‚’è¿½åŠ 
            race_info['race_id'] = race_id
            
        except Exception as e:
            print(f"âš ï¸ ãƒ¬ãƒ¼ã‚¹æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return race_info
    
    def extract_horse_data(self, soup) -> List[Dict[str, Any]]:
        """å‡ºèµ°é¦¬ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        horses_data = []
        
        try:
            main_table = soup.find("table", {"class": "race_table_01 nk_tb_common"})
            if not main_table:
                print("âŒ ãƒ¡ã‚¤ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return horses_data
            
            for row in main_table.find_all("tr")[1:]:  # ãƒ˜ãƒƒãƒ€è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                cols = row.find_all("td")
                if len(cols) < 15:
                    continue
                
                horse_data = {}
                
                try:
                    # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿
                    horse_data['é¦¬ç•ª'] = cols[2].text.strip()
                    horse_data['é¦¬å'] = cols[3].text.strip()
                    horse_data['æ€§é½¢'] = cols[4].text.strip()
                    horse_data['æ–¤é‡'] = cols[5].text.strip()
                    horse_data['é¨æ‰‹'] = cols[6].text.strip()
                    
                    # é¦¬ä½“é‡å‡¦ç†
                    weight_text = cols[14].text.strip()
                    if '(' in weight_text and ')' in weight_text:
                        weight_parts = weight_text.split('(')
                        horse_data['é¦¬ä½“é‡'] = weight_parts[0].strip()
                        horse_data['é¦¬ä½“é‡å¤‰åŒ–'] = weight_parts[1].replace(')', '').strip()
                    else:
                        horse_data['é¦¬ä½“é‡'] = weight_text
                        horse_data['é¦¬ä½“é‡å¤‰åŒ–'] = '0'
                    
                    # ã‚ªãƒƒã‚º
                    horse_data['å˜å‹ã‚ªãƒƒã‚º'] = cols[12].text.strip()
                    
                    # äººæ°—
                    horse_data['äººæ°—'] = cols[13].text.strip() if len(cols) > 13 else ''
                    
                    # æ ç•ªï¼ˆé¦¬ç•ªã‹ã‚‰è¨ˆç®—ï¼‰
                    try:
                        horse_num = int(horse_data['é¦¬ç•ª'])
                        horse_data['æ '] = ((horse_num - 1) // 2) + 1
                    except:
                        horse_data['æ '] = 0
                    
                    # èª¿æ•™å¸«
                    if len(cols) > 18:
                        trainer_elem = cols[18].find('a')
                        horse_data['èª¿æ•™å¸«'] = trainer_elem.text.strip() if trainer_elem else ''
                    else:
                        horse_data['èª¿æ•™å¸«'] = ''
                    
                    horses_data.append(horse_data)
                    
                except Exception as e:
                    print(f"âš ï¸ é¦¬ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                    continue
        
        except Exception as e:
            print(f"âŒ å‡ºèµ°é¦¬ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return horses_data
    
    def scrape_race(self, race_id: str) -> Optional[Dict[str, Any]]:
        """æŒ‡å®šã•ã‚ŒãŸrace_idã®ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å®Œå…¨å–å¾—"""
        print(f"ğŸ‡ ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹: {race_id}")
        
        # HTMLãƒ‡ãƒ¼ã‚¿å–å¾—
        content = self.fetch_race_data(race_id)
        if not content:
            return None
        
        try:
            soup = BeautifulSoup(content, "html.parser", from_encoding="euc-jp")
            
            # ãƒ¬ãƒ¼ã‚¹åŸºæœ¬æƒ…å ±
            race_info = self.extract_race_info(soup, race_id)
            
            # å‡ºèµ°é¦¬ãƒ‡ãƒ¼ã‚¿
            horses_data = self.extract_horse_data(soup)
            
            # ã‚ªãƒƒã‚ºãƒ‡ãƒ¼ã‚¿
            odds_data = self.extract_odds_data(soup)
            
            result = {
                'race_info': race_info,
                'horses': horses_data,
                'odds': odds_data,
                'horse_count': len(horses_data)
            }
            
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†")
            print(f"   ãƒ¬ãƒ¼ã‚¹å: {race_info['race_name']}")
            print(f"   å‡ºèµ°é ­æ•°: {len(horses_data)}é ­")
            print(f"   è·é›¢: {race_info['distance']}m")
            print(f"   ã‚³ãƒ¼ã‚¹: {race_info['surface']}{race_info['direction']}")
            
            return result
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿è§£æã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def save_to_csv(self, race_data: Dict[str, Any], filename: Optional[str] = None) -> str:
        """ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if not filename:
            race_id = race_data['race_info']['race_id']
            filename = f"live_race_data_{race_id}.csv"
        
        try:
            # å‡ºèµ°é¦¬ãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›
            df = pd.DataFrame(race_data['horses'])
            
            # ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã‚’å„è¡Œã«è¿½åŠ 
            race_info = race_data['race_info']
            for key, value in race_info.items():
                if key not in df.columns:
                    df[key] = value
            
            # CSVã«ä¿å­˜
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            
            print(f"ğŸ’¾ CSVãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {filename}")
            print(f"   è¡Œæ•°: {len(df)}")
            print(f"   åˆ—æ•°: {len(df.columns)}")
            
            return filename
            
        except Exception as e:
            print(f"âŒ CSVä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return ""
    
    def save_to_json(self, race_data: Dict[str, Any], filename: Optional[str] = None) -> str:
        """ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if not filename:
            race_id = race_data['race_info']['race_id']
            filename = f"live_race_data_{race_id}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(race_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ JSONãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {filename}")
            return filename
            
        except Exception as e:
            print(f"âŒ JSONä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return ""


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description='ãƒ©ã‚¤ãƒ–ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼')
    parser.add_argument('race_id', help='ãƒ¬ãƒ¼ã‚¹ID (ä¾‹: 202505021212)')
    parser.add_argument('--output', choices=['csv', 'json', 'both'], default='csv', 
                       help='å‡ºåŠ›å½¢å¼ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: csv)')
    parser.add_argument('--filename', help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ‹¡å¼µå­ãªã—ï¼‰')
    parser.add_argument('--timeout', type=int, default=10, help='ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç§’æ•°')
    
    args = parser.parse_args()
    
    # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    if len(args.race_id) != 12 or not args.race_id.isdigit():
        print("âŒ ç„¡åŠ¹ãªãƒ¬ãƒ¼ã‚¹IDå½¢å¼ã§ã™ã€‚12æ¡ã®æ•°å­—ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        print("   ä¾‹: 202505021212")
        sys.exit(1)
    
    print("ğŸ‡ ãƒ©ã‚¤ãƒ–ãƒ¬ãƒ¼ã‚¹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼é–‹å§‹")
    print(f"   ãƒ¬ãƒ¼ã‚¹ID: {args.race_id}")
    print(f"   å‡ºåŠ›å½¢å¼: {args.output}")
    
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
    scraper = LiveRaceScraper(timeout=args.timeout)
    race_data = scraper.scrape_race(args.race_id)
    
    if not race_data:
        print("âŒ ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    saved_files = []
    
    if args.output in ['csv', 'both']:
        csv_filename = args.filename + '.csv' if args.filename else None
        csv_file = scraper.save_to_csv(race_data, csv_filename)
        if csv_file:
            saved_files.append(csv_file)
    
    if args.output in ['json', 'both']:
        json_filename = args.filename + '.json' if args.filename else None
        json_file = scraper.save_to_json(race_data, json_filename)
        if json_file:
            saved_files.append(json_file)
    
    print(f"\nğŸ‰ å‡¦ç†å®Œäº†ï¼")
    if saved_files:
        print("ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
        for file in saved_files:
            print(f"  ğŸ“„ {file}")
    
    # ç°¡å˜ãªçµ±è¨ˆè¡¨ç¤º
    print(f"\nğŸ“Š ãƒ¬ãƒ¼ã‚¹æ¦‚è¦:")
    print(f"   ãƒ¬ãƒ¼ã‚¹å: {race_data['race_info']['race_name']}")
    print(f"   é–‹å‚¬æ—¥: {race_data['race_info']['date']}")
    print(f"   é–‹å‚¬å ´: {race_data['race_info']['place']}")
    print(f"   è·é›¢: {race_data['race_info']['distance']}m")
    print(f"   å‡ºèµ°é ­æ•°: {race_data['horse_count']}é ­")


if __name__ == "__main__":
    main()