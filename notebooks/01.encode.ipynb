{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ファイル取得：開始\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/2022.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m yearList:\n\u001b[32m     52\u001b[39m     file_path = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdata/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSHIFT-JIS\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m     df[\u001b[33m'\u001b[39m\u001b[33m日付\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(df[\u001b[33m'\u001b[39m\u001b[33m日付\u001b[39m\u001b[33m'\u001b[39m], \u001b[38;5;28mformat\u001b[39m=\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY年\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm月\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m日\u001b[39m\u001b[33m'\u001b[39m, errors=\u001b[33m'\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     55\u001b[39m     df[\u001b[33m'\u001b[39m\u001b[33m着順\u001b[39m\u001b[33m'\u001b[39m] = pd.to_numeric(df[\u001b[33m'\u001b[39m\u001b[33m着順\u001b[39m\u001b[33m'\u001b[39m], errors=\u001b[33m'\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Keiba_AI/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Keiba_AI/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Keiba_AI/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Keiba_AI/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Keiba_AI/.venv/lib/python3.13/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/2022.csv'"
     ]
    }
   ],
   "source": [
    "## 00.data_scraping.ipynbのスクレイピングデータを抽出、加工する。馬ごとの近5走における各特徴量データを追加。\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "def class_mapping(row):\n",
    "    # クラス名を数値にマッピング\n",
    "    mappings = {'障害': 0, 'G1': 10, 'G2': 9, 'G3': 8, '(L)': 7, 'オープン': 7, 'OP': 7, '3勝': 6, '1600': 6, '2勝': 5, '1000': 5, '1勝': 4, '500': 4, '新馬': 3, '未勝利': 1}\n",
    "    for key, value in mappings.items():\n",
    "        if key in row:\n",
    "            return value\n",
    "    return 0\n",
    "\n",
    "def standardize_times(df, col_name):\n",
    "    # 走破時間を秒に変換\n",
    "    time_parts = df[col_name].str.split(':', expand=True)\n",
    "    seconds = time_parts[0].astype(float) * 60 + time_parts[1].str.split('.', expand=True)[0].astype(float) + time_parts[1].str.split('.', expand=True)[1].astype(float) / 10\n",
    "    seconds = seconds.ffill()\n",
    "    \n",
    "    # 平均と標準偏差を計算し標準化\n",
    "    mean_seconds = seconds.mean()\n",
    "    std_seconds = seconds.std()\n",
    "    df[col_name] = -((seconds - mean_seconds) / std_seconds)\n",
    "    \n",
    "    # 外れ値処理\n",
    "    df[col_name] = df[col_name].apply(lambda x: -3 if x < -3 else (2 if x > 2.5 else x))\n",
    "    \n",
    "    # 再度標準化\n",
    "    mean_seconds_2 = df[col_name].mean()\n",
    "    std_seconds_2 = df[col_name].std()\n",
    "    df[col_name] = (df[col_name] - mean_seconds_2) / std_seconds_2\n",
    "    \n",
    "    return mean_seconds, std_seconds, mean_seconds_2, std_seconds_2\n",
    "\n",
    "def add_seasonal_features(df, date_columns):\n",
    "    # 日付カラムから季節特徴量を追加\n",
    "    for date_col in date_columns:\n",
    "        if not np.issubdtype(df[date_col].dtype, np.datetime64):\n",
    "            df[date_col] = pd.to_datetime(df[date_col])\n",
    "        df[f'{date_col}_sin'] = np.sin((df[date_col].dt.month - 1) * (2 * np.pi / 12))\n",
    "        df[f'{date_col}_cos'] = np.cos((df[date_col].dt.month - 1) * (2 * np.pi / 12))\n",
    "\n",
    "yearStart = 2022\n",
    "yearEnd = 2023\n",
    "yearList = np.arange(yearStart, yearEnd + 1)\n",
    "dfs = []\n",
    "\n",
    "print(\"ファイル取得：開始\")\n",
    "\n",
    "for year in yearList:\n",
    "    file_path = f\"data/{year}.csv\"\n",
    "    df = pd.read_csv(file_path, encoding=\"SHIFT-JIS\", header=0)\n",
    "    df['日付'] = pd.to_datetime(df['日付'], format='%Y年%m月%d日', errors='coerce')\n",
    "    df['着順'] = pd.to_numeric(df['着順'], errors='coerce')\n",
    "    df = df.dropna(subset=['着順'])\n",
    "    df['着順'] = df['着順'].astype(int)\n",
    "    df['賞金'] = pd.to_numeric(df['賞金'], errors='coerce').fillna(0)\n",
    "    dfs.append(df)\n",
    "\n",
    "df_combined = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(\"ファイル取得：完了\")\n",
    "print(\"データ変換：開始\")\n",
    "\n",
    "# NaNが含まれる行を削除\n",
    "df_combined = df_combined.dropna(subset=['走破時間'])\n",
    "\n",
    "# 走破時間の標準化\n",
    "mean_seconds, std_seconds, mean_seconds_2, std_seconds_2 = standardize_times(df_combined, '走破時間')\n",
    "\n",
    "print('1回目平均' + str(mean_seconds))\n",
    "print('2回目平均' + str(mean_seconds_2))\n",
    "print('1回目標準偏差' + str(std_seconds))\n",
    "print('2回目標準偏差' + str(std_seconds_2))\n",
    "\n",
    "# 標準化情報をCSVに保存\n",
    "time_df = pd.DataFrame({\n",
    "    'Mean': [mean_seconds, mean_seconds_2],\n",
    "    'Standard Deviation': [std_seconds, std_seconds_2]\n",
    "}, index=['First Time', 'Second Time'])\n",
    "time_df.to_csv('config/standard_deviation.csv')\n",
    "\n",
    "# 通過順の平均を計算\n",
    "pas = df_combined['通過順'].str.split('-', expand=True)\n",
    "df_combined['通過順'] = pas.astype(float).mean(axis=1)\n",
    "\n",
    "# マッピング情報を適用\n",
    "mappings = {\n",
    "    '性': {'牡': 0, '牝': 1, 'セ': 2},\n",
    "    '芝・ダート': {'芝': 0, 'ダ': 1, '障': 2},\n",
    "    '回り': {'右': 0, '左': 1, '芝': 2, '直': 2},\n",
    "    '馬場': {'良': 0, '稍': 1, '重': 2, '不': 3},\n",
    "    '天気': {'晴': 0, '曇': 1, '小': 2, '雨': 3, '雪': 4}\n",
    "}\n",
    "for column, mapping in mappings.items():\n",
    "    df_combined[column] = df_combined[column].map(mapping)\n",
    "\n",
    "# クラス変換を適用\n",
    "df_combined['クラス'] = df_combined['クラス'].apply(class_mapping)\n",
    "\n",
    "print(\"データ変換：完了\")\n",
    "print(\"近5走取得：開始\")\n",
    "\n",
    "# データをソート\n",
    "df_combined.sort_values(by=['馬', '日付'], ascending=[True, False], inplace=True)\n",
    "\n",
    "features = ['馬番', '騎手', '斤量', 'オッズ', '体重', '体重変化', '上がり', '通過順', '着順', '距離', 'クラス', '走破時間', '芝・ダート', '天気', '馬場']\n",
    "\n",
    "# 各馬の過去5走の情報をシフトしながら取得し、ffillで欠損値を補完\n",
    "shifts = {}\n",
    "for i in range(1, 6):\n",
    "    shifts[f'日付{i}'] = df_combined.groupby('馬')['日付'].shift(-i)\n",
    "    for feature in features:\n",
    "        shifts[f'{feature}{i}'] = df_combined.groupby('馬')[feature].shift(-i).ffill()\n",
    "\n",
    "# 新しい列を一度にDataFrameに追加\n",
    "df_combined = pd.concat([df_combined, pd.DataFrame(shifts)], axis=1)\n",
    "\n",
    "# race_idと馬でグルーピングし、最新の特徴量を取得\n",
    "df_combined = df_combined.groupby(['race_id', '馬'], as_index=False).last()\n",
    "df_combined.sort_values(by='race_id', ascending=False, inplace=True)\n",
    "\n",
    "print(\"近5走取得：終了\")\n",
    "print(\"日付変換：開始\")\n",
    "\n",
    "df_combined.replace('---', np.nan, inplace=True)\n",
    "\n",
    "# 距離差と日付差を計算\n",
    "df_combined['距離差'] = df_combined['距離'] - df_combined['距離1']\n",
    "df_combined['日付差'] = (df_combined['日付'] - df_combined['日付1']).dt.days\n",
    "for i in range(1, 5):\n",
    "    df_combined[f'距離差{i}'] = df_combined[f'距離{i}'] - df_combined[f'距離{i+1}']\n",
    "    df_combined[f'日付差{i}'] = (df_combined[f'日付{i}'] - df_combined[f'日付{i+1}']).dt.days\n",
    "\n",
    "# 斤量関連の列を数値に変換し、変換できないデータはNaNに\n",
    "kinryo_columns = ['斤量', '斤量1', '斤量2', '斤量3', '斤量4', '斤量5']\n",
    "df_combined[kinryo_columns] = df_combined[kinryo_columns].apply(pd.to_numeric, errors='coerce')\n",
    "df_combined['平均斤量'] = df_combined[kinryo_columns].mean(axis=1)\n",
    "\n",
    "# 騎手の勝率を計算し、保存およびマージ\n",
    "jockey_win_rate = df_combined.groupby('騎手')['着順'].apply(lambda x: (x == 1).sum() / x.count()).reset_index()\n",
    "jockey_win_rate.columns = ['騎手', '騎手の勝率']\n",
    "jockey_win_rate.to_csv('calc_rate/jockey_win_rate.csv', index=False)\n",
    "df_combined = pd.merge(df_combined, jockey_win_rate, on='騎手', how='left')\n",
    "\n",
    "# 各レースの出走頭数を計算\n",
    "df_combined['出走頭数'] = df_combined.groupby('race_id')['race_id'].transform('count')\n",
    "\n",
    "# 各馬に対して過去5レースの出走頭数を特徴量として追加\n",
    "for i in range(1, 6):\n",
    "    df_combined[f'出走頭数{i}'] = df_combined.groupby('馬')['出走頭数'].shift(i).fillna(0)\n",
    "\n",
    "# 距離と走破時間からスピードを計算し、平均スピードを新しい列として追加\n",
    "for i in range(1, 6):\n",
    "    df_combined[f'スピード{i}'] = df_combined[f'距離{i}'] / df_combined[f'走破時間{i}']\n",
    "df_combined['平均スピード'] = df_combined[[f'スピード{i}' for i in range(1, 6)]].mean(axis=1, skipna=True)\n",
    "df_combined.drop(columns=[f'スピード{i}' for i in range(1, 6)], inplace=True)\n",
    "\n",
    "# 過去5走の賞金を取得し、賞金合計を計算\n",
    "for i in range(1, 6):\n",
    "    df_combined[f'賞金{i}'] = df_combined.groupby('馬')['賞金'].shift(i)\n",
    "df_combined['過去5走の合計賞金'] = df_combined[[f'賞金{i}' for i in range(1, 6)]].sum(axis=1)\n",
    "df_combined.drop(columns=[f'賞金{i}' for i in range(1, 6)] + ['賞金'], inplace=True)\n",
    "\n",
    "df_combined.sort_values(by='race_id', ascending=False, inplace=True)\n",
    "\n",
    "# 日付カラムから年、月、日を抽出\n",
    "df_combined['year'] = df_combined['日付'].dt.year\n",
    "df_combined['month'] = df_combined['日付'].dt.month\n",
    "df_combined['day'] = df_combined['日付'].dt.day\n",
    "\n",
    "# 季節特徴量を追加\n",
    "date_columns = ['日付1', '日付2', '日付3', '日付4', '日付5']\n",
    "add_seasonal_features(df_combined, date_columns)\n",
    "\n",
    "date_columns = ['日付', '日付1', '日付2', '日付3', '日付4', '日付5']\n",
    "for col in date_columns:\n",
    "    df_combined['year'] = df_combined[col].dt.year\n",
    "    df_combined['month'] = df_combined[col].dt.month\n",
    "    df_combined['day'] = df_combined[col].dt.day\n",
    "    df_combined[col] = (df_combined['year'] - yearStart) * 365 + df_combined['month'] * 30 + df_combined['day']\n",
    "df_combined.drop(['year', 'month', 'day'], axis=1, inplace=True)\n",
    "\n",
    "print(\"日付変換：終了\")\n",
    "\n",
    "# 騎手の乗り替わり特徴量を追加\n",
    "df_combined['騎手の乗り替わり'] = df_combined.groupby('馬')['騎手'].transform(lambda x: (x != x.shift()).astype(int))\n",
    "\n",
    "# カテゴリカル変数のラベルエンコーディング\n",
    "categorical_features = ['馬', '騎手', '調教師', 'レース名', '開催', '場名', '騎手1', '騎手2', '騎手3', '騎手4', '騎手5']\n",
    "for i, feature in enumerate(categorical_features):\n",
    "    print(f\"\\rProcessing feature {i+1}/{len(categorical_features)}\", end=\"\")\n",
    "    le = LabelEncoder()\n",
    "    df_combined[feature] = le.fit_transform(df_combined[feature].astype(str))\n",
    "\n",
    "# エンコーディングとスケーリング後のデータを確認\n",
    "print(\"ファイル出力：開始\")\n",
    "df_combined.to_csv(f'encoded/{yearStart}_{yearEnd}encoded_data.csv', index=False)\n",
    "print(\"ファイル出力：終了\")\n",
    "\n",
    "\n",
    "\n",
    "# # データの読み込み\n",
    "# data = pd.read_csv('encoded/encoded_data.csv')\n",
    "\n",
    "# # 特徴量エンジニアリングの追加 ----------------------------------------\n",
    "# # 騎手と調教師の組み合わせ（列が存在する場合）\n",
    "# if '騎手' in data.columns and '調教師' in data.columns:\n",
    "#     data['騎手_調教師'] = data['騎手'] + '_' + data['調教師']\n",
    "\n",
    "# # 過去3走の平均順位（馬の履歴データがある場合）\n",
    "# if '着順' in data.columns and '馬' in data.columns:\n",
    "#     data['過去3走平均順位'] = data.groupby('馬')['着順'].transform(\n",
    "#         lambda x: x.rolling(3, min_periods=1).mean()  # 最低1走分あれば計算\n",
    "#     )\n",
    "# # ------------------------------------------------------------\n",
    "\n",
    "# # 着順を変換\n",
    "# data['着順'] = data['着順'].map(lambda x: 1 if x < 4 else 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
