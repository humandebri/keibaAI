#!/usr/bin/env python3
"""
netkeiba.com ãƒ¬ãƒ¼ã‚¹é–‹å‚¬æ—¥ç¨‹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
ä»Šæ—¥ãƒ»æ˜æ—¥ãƒ»ä»Šé€±æœ«ã®ãƒ¬ãƒ¼ã‚¹ä¸€è¦§ã‚’å–å¾—
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
from datetime import datetime, timedelta
from typing import List, Dict


class RaceScheduleScraper:
    """netkeiba.com ã‹ã‚‰ãƒ¬ãƒ¼ã‚¹é–‹å‚¬æ—¥ç¨‹ã‚’å–å¾—"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        self.base_url = "https://race.netkeiba.com"
        
        # ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰
        self.course_codes = {
            '01': 'æœ­å¹Œ', '02': 'å‡½é¤¨', '03': 'ç¦å³¶', '04': 'æ–°æ½Ÿ',
            '05': 'æ±äº¬', '06': 'ä¸­å±±', '07': 'ä¸­äº¬', '08': 'äº¬éƒ½',
            '09': 'é˜ªç¥', '10': 'å°å€‰', '11': 'æœªç¢ºå®š', '12': 'æ±äº¬',
            '13': 'ç››å²¡', '14': 'å¤§äº•', '15': 'èˆ¹æ©‹', '16': 'å·å´', '17': 'æµ¦å’Œ'
        }
    
    def get_today_races(self) -> pd.DataFrame:
        """ä»Šæ—¥ã®ãƒ¬ãƒ¼ã‚¹ä¸€è¦§ã‚’å–å¾—"""
        today = datetime.now()
        return self._get_races_by_date(today)
    
    def get_tomorrow_races(self) -> pd.DataFrame:
        """æ˜æ—¥ã®ãƒ¬ãƒ¼ã‚¹ä¸€è¦§ã‚’å–å¾—"""
        tomorrow = datetime.now() + timedelta(days=1)
        return self._get_races_by_date(tomorrow)
    
    def get_weekend_races(self) -> pd.DataFrame:
        """ä»Šé€±æœ«(åœŸæ—¥)ã®ãƒ¬ãƒ¼ã‚¹ä¸€è¦§ã‚’å–å¾—"""
        today = datetime.now()
        
        # ä»Šé€±ã®åœŸæ›œæ—¥ã‚’è¨ˆç®—
        days_until_saturday = (5 - today.weekday()) % 7
        if days_until_saturday == 0 and today.weekday() == 5:  # ä»Šæ—¥ãŒåœŸæ›œæ—¥
            saturday = today
        else:
            saturday = today + timedelta(days=days_until_saturday)
        
        sunday = saturday + timedelta(days=1)
        
        # åœŸæ—¥ã®ãƒ¬ãƒ¼ã‚¹ã‚’å–å¾—
        saturday_races = self._get_races_by_date(saturday)
        sunday_races = self._get_races_by_date(sunday)
        
        # çµåˆ
        all_races = pd.concat([saturday_races, sunday_races], ignore_index=True)
        return all_races.sort_values(['æ—¥ä»˜', 'ç«¶é¦¬å ´', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·'])
    
    def _get_races_by_date(self, target_date: datetime) -> pd.DataFrame:
        """æŒ‡å®šæ—¥ã®ãƒ¬ãƒ¼ã‚¹ä¸€è¦§ã‚’å–å¾—"""
        date_str = target_date.strftime('%Y%m%d')
        url = f"{self.base_url}/top/race_list_sub.html?kaisai_date={date_str}"
        
        print(f"ğŸ“… {target_date.strftime('%Yå¹´%mæœˆ%dæ—¥')}ã®ãƒ¬ãƒ¼ã‚¹æƒ…å ±å–å¾—ä¸­...")
        
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            races = self._parse_race_list(soup, target_date)
            print(f"âœ… {len(races)}ãƒ¬ãƒ¼ã‚¹ã‚’å–å¾—")
            return pd.DataFrame(races)
            
        except Exception as e:
            print(f"âŒ {target_date.strftime('%Yå¹´%mæœˆ%dæ—¥')}ã®ãƒ¬ãƒ¼ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return pd.DataFrame()
    
    def _parse_race_list(self, soup: BeautifulSoup, target_date: datetime) -> List[Dict]:
        """ãƒ¬ãƒ¼ã‚¹ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º"""
        races = []
        
        # ç«¶é¦¬å ´ã”ã¨ã®ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã‚’æ¢ã™
        race_sections = soup.find_all('div', class_='race_kaisai_info')
        if not race_sections:
            # åˆ¥ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™
            race_sections = soup.find_all('div', class_='kaisai_info')
        
        for section in race_sections:
            # ç«¶é¦¬å ´åã‚’å–å¾—
            course_name = self._extract_course_name(section)
            if not course_name:
                continue
            
            # ãã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã®ãƒ¬ãƒ¼ã‚¹ãƒªã‚¹ãƒˆã‚’å–å¾—
            race_items = section.find_all('a', href=re.compile(r'/race/shutuba\.html\?race_id='))
            
            for race_item in race_items:
                race_info = self._extract_race_info(race_item, course_name, target_date)
                if race_info:
                    races.append(race_info)
        
        # ãƒ¬ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€åˆ¥ã®æ–¹æ³•ã§æ¢ã™
        if not races:
            races = self._fallback_race_extraction(soup, target_date)
        
        return races
    
    def _extract_course_name(self, section) -> str:
        """ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰ç«¶é¦¬å ´åã‚’æŠ½å‡º"""
        # ç«¶é¦¬å ´åã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¤‡æ•°è©¦ã™
        patterns = [
            section.find('h3'),
            section.find('h2'),
            section.find('div', class_='kaisai_name'),
            section.find('span', class_='kaisai_name'),
        ]
        
        for elem in patterns:
            if elem:
                text = elem.get_text(strip=True)
                # ç«¶é¦¬å ´åã‚’æŠ½å‡ºï¼ˆä¾‹ï¼šã€Œæ±äº¬ç«¶é¦¬å ´ã€â†’ã€Œæ±äº¬ã€ï¼‰
                for course_code, course_name in self.course_codes.items():
                    if course_name in text:
                        return course_name
        
        return ""
    
    def _extract_race_info(self, race_item, course_name: str, target_date: datetime) -> Dict:
        """å€‹åˆ¥ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã‚’æŠ½å‡º"""
        try:
            # race_idã‚’URLã‹ã‚‰æŠ½å‡º
            href = race_item.get('href', '')
            race_id_match = re.search(r'race_id=(\d{12})', href)
            if not race_id_match:
                return None
            
            race_id = race_id_match.group(1)
            
            # ãƒ¬ãƒ¼ã‚¹ç•ªå·ã‚’æŠ½å‡º
            race_text = race_item.get_text(strip=True)
            race_num_match = re.search(r'(\d{1,2})R', race_text)
            race_num = int(race_num_match.group(1)) if race_num_match else 0
            
            # ãƒ¬ãƒ¼ã‚¹åã‚’æŠ½å‡º
            race_name = race_text.replace(f'{race_num}R', '').strip() if race_num else race_text
            
            # ç™ºèµ°æ™‚åˆ»ã‚’æŠ½å‡º
            time_match = re.search(r'(\d{1,2}):(\d{2})', race_text)
            start_time = f"{time_match.group(1)}:{time_match.group(2)}" if time_match else "ä¸æ˜"
            
            return {
                'race_id': race_id,
                'æ—¥ä»˜': target_date.strftime('%Y-%m-%d'),
                'æ›œæ—¥': target_date.strftime('%A'),
                'ç«¶é¦¬å ´': course_name,
                'ãƒ¬ãƒ¼ã‚¹ç•ªå·': race_num,
                'ãƒ¬ãƒ¼ã‚¹å': race_name,
                'ç™ºèµ°æ™‚åˆ»': start_time,
                'URL': f"{self.base_url}{href}"
            }
            
        except Exception as e:
            print(f"âš ï¸ ãƒ¬ãƒ¼ã‚¹æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _fallback_race_extraction(self, soup: BeautifulSoup, target_date: datetime) -> List[Dict]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šåˆ¥ã®æ–¹æ³•ã§ãƒ¬ãƒ¼ã‚¹ã‚’æ¢ã™"""
        races = []
        
        # å…¨ã¦ã®race_idãƒªãƒ³ã‚¯ã‚’æ¢ã™
        race_links = soup.find_all('a', href=re.compile(r'race_id=\d{12}'))
        
        for link in race_links:
            href = link.get('href', '')
            race_id_match = re.search(r'race_id=(\d{12})', href)
            if race_id_match:
                race_id = race_id_match.group(1)
                
                # race_idã‹ã‚‰ç«¶é¦¬å ´ã‚’æ¨æ¸¬
                course_code = race_id[8:10]
                course_name = self.course_codes.get(course_code, 'ä¸æ˜')
                
                race_text = link.get_text(strip=True)
                
                races.append({
                    'race_id': race_id,
                    'æ—¥ä»˜': target_date.strftime('%Y-%m-%d'),
                    'æ›œæ—¥': target_date.strftime('%A'),
                    'ç«¶é¦¬å ´': course_name,
                    'ãƒ¬ãƒ¼ã‚¹ç•ªå·': 0,
                    'ãƒ¬ãƒ¼ã‚¹å': race_text,
                    'ç™ºèµ°æ™‚åˆ»': 'ä¸æ˜',
                    'URL': f"{self.base_url}{href}"
                })
        
        return races


def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ãƒ¬ãƒ¼ã‚¹é–‹å‚¬æ—¥ç¨‹å–å¾—')
    parser.add_argument('--target', choices=['today', 'tomorrow', 'weekend'], 
                       default='today', help='å–å¾—å¯¾è±¡')
    parser.add_argument('--output', type=str, default='race_schedule.csv', 
                       help='å‡ºåŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«')
    
    args = parser.parse_args()
    
    scraper = RaceScheduleScraper()
    
    if args.target == 'today':
        races = scraper.get_today_races()
    elif args.target == 'tomorrow':
        races = scraper.get_tomorrow_races()
    else:  # weekend
        races = scraper.get_weekend_races()
    
    if races.empty:
        print("âŒ ãƒ¬ãƒ¼ã‚¹æƒ…å ±ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    # CSVä¿å­˜
    races.to_csv(args.output, index=False, encoding='utf-8')
    print(f"\nğŸ’¾ ãƒ¬ãƒ¼ã‚¹æ—¥ç¨‹ä¿å­˜: {args.output}")
    
    # çµæœè¡¨ç¤º
    print(f"\nğŸ“Š å–å¾—çµæœ: {len(races)}ãƒ¬ãƒ¼ã‚¹")
    print("\nğŸ‡ ãƒ¬ãƒ¼ã‚¹ä¸€è¦§:")
    for _, race in races.iterrows():
        print(f"  {race['æ—¥ä»˜']} {race['ç«¶é¦¬å ´']} {race['ãƒ¬ãƒ¼ã‚¹ç•ªå·']:2d}R "
              f"{race['ç™ºèµ°æ™‚åˆ»']} {race['ãƒ¬ãƒ¼ã‚¹å']}")
        print(f"    ID: {race['race_id']}")


if __name__ == "__main__":
    main()