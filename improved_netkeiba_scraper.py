#!/usr/bin/env python3
"""
æ”¹è‰¯ç‰ˆnetkeiba.comã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 
ä¸€èˆ¬çš„ãªnetkeiba.comã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ‰‹æ³•ã‚’å‚è€ƒã«ã—ãŸåŠ¹ç‡çš„ãªå®Ÿè£…
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import time
import random
from typing import Dict, List, Optional, Tuple
import urllib.parse


class ImprovedNetkeibaScrapor:
    """æ”¹è‰¯ç‰ˆnetkeiba.comã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self):
        self.session = requests.Session()
        self._setup_session()
        self.base_url = "https://race.netkeiba.com"
        
        # ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰
        self.place_codes = {
            "01": "æœ­å¹Œ", "02": "å‡½é¤¨", "03": "ç¦å³¶", "04": "æ–°æ½Ÿ", "05": "æ±äº¬",
            "06": "ä¸­å±±", "07": "ä¸­äº¬", "08": "äº¬éƒ½", "09": "é˜ªç¥", "10": "å°å€‰"
        }
        
    def _setup_session(self):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®šã‚’æœ€é©åŒ–"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
        }
        self.session.headers.update(headers)
        
    def scrape_race_complete(self, race_id: str) -> pd.DataFrame:
        """å®Œå…¨ãªãƒ¬ãƒ¼ã‚¹æƒ…å ±ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        print(f"ğŸ‡ æ”¹è‰¯ç‰ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {race_id}")
        
        # ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã‚’è§£æ
        race_info = self._parse_race_id(race_id)
        print(f"ğŸ“ {race_info['place']} {race_info['meeting']}å›{race_info['day']}æ—¥ç›® {race_info['race_num']}R")
        
        # 1. å‡ºé¦¬è¡¨ãƒšãƒ¼ã‚¸ã‹ã‚‰åŸºæœ¬æƒ…å ±å–å¾—
        print("ğŸ“‹ å‡ºé¦¬è¡¨ãƒšãƒ¼ã‚¸ã‹ã‚‰åŸºæœ¬æƒ…å ±å–å¾—ä¸­...")
        shutuba_data = self._scrape_shutuba_page(race_id)
        
        # 2. ã‚ªãƒƒã‚ºãƒšãƒ¼ã‚¸ã‹ã‚‰ç¢ºå®šã‚ªãƒƒã‚ºå–å¾—
        print("ğŸ’° ã‚ªãƒƒã‚ºãƒšãƒ¼ã‚¸ã‹ã‚‰ç¢ºå®šã‚ªãƒƒã‚ºå–å¾—ä¸­...")
        odds_data = self._scrape_odds_page(race_id)
        
        # 3. çµæœãƒšãƒ¼ã‚¸ã‹ã‚‰å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰
        print("ğŸ† çµæœãƒšãƒ¼ã‚¸ã‹ã‚‰å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")
        result_data = self._scrape_result_page(race_id)
        
        # 4. ãƒ‡ãƒ¼ã‚¿çµ±åˆ
        final_data = self._merge_all_data(shutuba_data, odds_data, result_data, race_id)
        
        print(f"âœ… æ”¹è‰¯ç‰ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: {len(final_data)}é ­")
        return final_data
    
    def _parse_race_id(self, race_id: str) -> Dict:
        """ãƒ¬ãƒ¼ã‚¹IDã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º"""
        year = race_id[:4]
        place_code = race_id[4:6]
        meeting = race_id[6:8]
        day = race_id[8:10]
        race_num = race_id[10:12]
        
        return {
            'year': year,
            'place_code': place_code,
            'place': self.place_codes.get(place_code, f"ä¸æ˜({place_code})"),
            'meeting': meeting,
            'day': day,
            'race_num': race_num
        }
    
    def _scrape_shutuba_page(self, race_id: str) -> pd.DataFrame:
        """å‡ºé¦¬è¡¨ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        url = f"{self.base_url}/race/shutuba.html?race_id={race_id}"
        
        try:
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ã‚’èª¿æ•´
            time.sleep(random.uniform(1.0, 2.0))
            
            response = self.session.get(url, timeout=20)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Shutuba_Tableã‚’æ¤œç´¢
            shutuba_table = soup.find('table', class_='Shutuba_Table')
            if not shutuba_table:
                print("âŒ Shutuba_TableãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return pd.DataFrame()
            
            print("âœ“ Shutuba_Tableç™ºè¦‹ã€ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºä¸­...")
            
            horses_data = []
            rows = shutuba_table.find_all('tr')
            
            for row_idx, row in enumerate(rows):
                cells = row.find_all(['td', 'th'])
                
                # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                if len(cells) < 8 or not cells[1].get_text(strip=True).isdigit():
                    continue
                
                horse_data = self._extract_shutuba_horse_data(cells, race_id, row_idx)
                if horse_data:
                    horses_data.append(horse_data)
            
            df = pd.DataFrame(horses_data)
            print(f"âœ“ å‡ºé¦¬è¡¨ãƒ‡ãƒ¼ã‚¿å–å¾—: {len(df)}é ­")
            return df
            
        except Exception as e:
            print(f"âŒ å‡ºé¦¬è¡¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
            return pd.DataFrame()
    
    def _extract_shutuba_horse_data(self, cells: List, race_id: str, row_idx: int) -> Optional[Dict]:
        """å‡ºé¦¬è¡¨ã‹ã‚‰é¦¬ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        try:
            # ãƒ‡ãƒãƒƒã‚°ç”¨: ã‚»ãƒ«å†…å®¹ã‚’ç¢ºèª
            if row_idx <= 3:  # æœ€åˆã®3é ­ã®ã¿ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
                cell_contents = [f"'{cell.get_text(strip=True)}'" for cell in cells[:10]]
                print(f"  è¡Œ{row_idx}: {cell_contents}")
            
            # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            data = {}
            
            # æ ç•ªï¼ˆã‚»ãƒ«0ã¾ãŸã¯1ï¼‰
            waku = self._extract_waku(cells)
            data['æ '] = waku
            
            # é¦¬ç•ªï¼ˆã‚»ãƒ«1ã¾ãŸã¯2ï¼‰
            umaban = self._extract_umaban(cells)
            if not umaban:
                return None
            data['é¦¬ç•ª'] = umaban
            
            # é¦¬åï¼ˆé€šå¸¸ã‚»ãƒ«3ã€ãƒªãƒ³ã‚¯ã¾ãŸã¯ãƒ†ã‚­ã‚¹ãƒˆï¼‰
            horse_name = self._extract_horse_name(cells)
            data['é¦¬å'] = horse_name
            
            # æ€§é½¢ï¼ˆé€šå¸¸ã‚»ãƒ«4ï¼‰
            sei_rei = self._extract_sei_rei(cells)
            data['æ€§é½¢'] = sei_rei
            
            # æ–¤é‡ï¼ˆé€šå¸¸ã‚»ãƒ«5ï¼‰
            kinryo = self._extract_kinryo(cells)
            data['æ–¤é‡'] = kinryo
            
            # é¨æ‰‹ï¼ˆé€šå¸¸ã‚»ãƒ«6ï¼‰
            jockey = self._extract_jockey(cells)
            data['é¨æ‰‹'] = jockey
            
            # å©èˆï¼ˆé€šå¸¸ã‚»ãƒ«7ï¼‰
            trainer = self._extract_trainer(cells)
            data['å©èˆ'] = trainer
            
            # é¦¬ä½“é‡ï¼ˆé€šå¸¸ã‚»ãƒ«8ï¼‰
            horse_weight = self._extract_horse_weight(cells)
            data['é¦¬ä½“é‡'] = horse_weight
            
            # ãƒ¬ãƒ¼ã‚¹IDè¿½åŠ 
            data['race_id'] = race_id
            
            return data
            
        except Exception as e:
            print(f"âš ï¸ è¡Œ{row_idx}ã®ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _extract_waku(self, cells: List) -> int:
        """æ ç•ªã‚’æŠ½å‡º"""
        # ã‚»ãƒ«0ã‹ã‚‰è©¦è¡Œ
        for i in [0, 1]:
            if i < len(cells):
                text = cells[i].get_text(strip=True)
                if text.isdigit() and 1 <= int(text) <= 8:
                    return int(text)
                
                # CSS classã‹ã‚‰æ¨æ¸¬
                classes = cells[i].get('class', [])
                for cls in classes:
                    if 'Waku' in cls:
                        match = re.search(r'(\d)', cls)
                        if match and 1 <= int(match.group(1)) <= 8:
                            return int(match.group(1))
        
        return 1  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    def _extract_umaban(self, cells: List) -> Optional[int]:
        """é¦¬ç•ªã‚’æŠ½å‡º"""
        for i in [1, 2, 0]:  # é€šå¸¸ã¯ã‚»ãƒ«1ã€å ´åˆã«ã‚ˆã£ã¦ã¯ã‚»ãƒ«2ã¾ãŸã¯ã‚»ãƒ«0
            if i < len(cells):
                text = cells[i].get_text(strip=True)
                if text.isdigit() and 1 <= int(text) <= 18:
                    return int(text)
        return None
    
    def _extract_horse_name(self, cells: List) -> str:
        """é¦¬åã‚’æŠ½å‡º"""
        # ã‚»ãƒ«3ï½5ã®ç¯„å›²ã§é¦¬åã‚’æ¢ã™
        for i in range(3, min(6, len(cells))):
            cell = cells[i]
            
            # ãƒªãƒ³ã‚¯ã‹ã‚‰é¦¬åã‚’å–å¾—ï¼ˆå„ªå…ˆï¼‰
            horse_link = cell.find('a', href=lambda href: href and 'horse' in href)
            if horse_link:
                name = horse_link.get_text(strip=True)
                if name and len(name) > 1:
                    return name
            
            # ã‚¯ãƒ©ã‚¹åã§åˆ¤å®š
            if 'HorseInfo' in str(cell.get('class', [])) or 'Horse_Name' in str(cell.get('class', [])):
                name = cell.get_text(strip=True)
                if name and len(name) > 1:
                    return name
            
            # spanã‚¿ã‚°å†…ã®é¦¬å
            span = cell.find('span')
            if span:
                name = span.get_text(strip=True)
                if name and len(name) > 1:
                    return name
        
        return "ä¸æ˜"
    
    def _extract_sei_rei(self, cells: List) -> str:
        """æ€§é½¢ã‚’æŠ½å‡º"""
        for i in range(4, min(7, len(cells))):
            text = cells[i].get_text(strip=True)
            # æ€§é½¢ã®ãƒ‘ã‚¿ãƒ¼ãƒ³: ç‰¡3, ç‰4, ã‚»5 ãªã©
            if re.match(r'^[ç‰¡ç‰ã‚»][0-9]$', text):
                return text
        return "ä¸æ˜"
    
    def _extract_kinryo(self, cells: List) -> float:
        """æ–¤é‡ã‚’æŠ½å‡º"""
        for i in range(5, min(8, len(cells))):
            text = cells[i].get_text(strip=True)
            # æ–¤é‡ã®ãƒ‘ã‚¿ãƒ¼ãƒ³: 57.0, 54.5 ãªã©
            if re.match(r'^5[0-9]\.[05]$', text):
                try:
                    return float(text)
                except:
                    pass
        return 57.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    def _extract_jockey(self, cells: List) -> str:
        """é¨æ‰‹ã‚’æŠ½å‡º"""
        for i in range(6, min(9, len(cells))):
            cell = cells[i]
            
            # ãƒªãƒ³ã‚¯ã‹ã‚‰é¨æ‰‹åã‚’å–å¾—
            jockey_link = cell.find('a', href=lambda href: href and 'jockey' in href)
            if jockey_link:
                name = jockey_link.get_text(strip=True)
                if name:
                    return name
            
            # ã‚¯ãƒ©ã‚¹åã§åˆ¤å®š
            if 'Jockey' in str(cell.get('class', [])):
                name = cell.get_text(strip=True)
                if name and len(name) > 1:
                    return name
        
        return "ä¸æ˜"
    
    def _extract_trainer(self, cells: List) -> str:
        """å©èˆã‚’æŠ½å‡º"""
        for i in range(7, min(10, len(cells))):
            cell = cells[i]
            
            # ãƒªãƒ³ã‚¯ã‹ã‚‰å©èˆåã‚’å–å¾—
            trainer_link = cell.find('a', href=lambda href: href and 'trainer' in href)
            if trainer_link:
                name = trainer_link.get_text(strip=True)
                if name:
                    # åœ°åŸŸãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’é™¤å»
                    return re.sub(r'^(æ —æ±|ç¾æµ¦|ç¬ æ¾|é‡‘æ²¢|åœ’ç”°|å§«è·¯|é«˜çŸ¥|ä½è³€|é–€åˆ¥|ç››å²¡|æ°´æ²¢|æµ¦å’Œ|èˆ¹æ©‹|å¤§äº•|å·å´)', '', name)
            
            # ã‚¯ãƒ©ã‚¹åã§åˆ¤å®š
            if 'Trainer' in str(cell.get('class', [])):
                name = cell.get_text(strip=True)
                if name and len(name) > 1:
                    return re.sub(r'^(æ —æ±|ç¾æµ¦)', '', name)
        
        return "ä¸æ˜"
    
    def _extract_horse_weight(self, cells: List) -> str:
        """é¦¬ä½“é‡ã‚’æŠ½å‡º"""
        for i in range(8, min(12, len(cells))):
            cell = cells[i]
            
            text = cell.get_text(strip=True)
            # é¦¬ä½“é‡ã®ãƒ‘ã‚¿ãƒ¼ãƒ³: 456(+2), 480(-4) ãªã©
            if re.match(r'\d{3,4}\([+-]?\d+\)', text):
                return text
            
            # ã‚¯ãƒ©ã‚¹åã§åˆ¤å®š
            if 'Weight' in str(cell.get('class', [])):
                text = cell.get_text(strip=True)
                if re.match(r'\d{3,4}', text):
                    return text
        
        return "ä¸æ˜"
    
    def _scrape_odds_page(self, race_id: str) -> Dict[int, Dict]:
        """ã‚ªãƒƒã‚ºãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        odds_data = {}
        
        # è¤‡æ•°ã®ã‚ªãƒƒã‚ºãƒšãƒ¼ã‚¸ã‚’è©¦è¡Œ
        odds_urls = [
            f"{self.base_url}/odds/index.html?race_id={race_id}",
            f"{self.base_url}/odds/win.html?race_id={race_id}",
        ]
        
        for url in odds_urls:
            try:
                time.sleep(random.uniform(1.5, 2.5))
                
                response = self.session.get(url, timeout=20)
                if response.status_code != 200:
                    continue
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ã‚ªãƒƒã‚ºãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã™
                odds_tables = soup.find_all('table')
                
                for table in odds_tables:
                    table_odds = self._extract_odds_from_table(table, race_id)
                    if table_odds:
                        print(f"âœ“ ã‚ªãƒƒã‚ºãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰{len(table_odds)}é ­ã®ãƒ‡ãƒ¼ã‚¿å–å¾—")
                        odds_data.update(table_odds)
                        break
                
                if odds_data:
                    break
                    
            except Exception as e:
                print(f"âš ï¸ ã‚ªãƒƒã‚ºãƒšãƒ¼ã‚¸ {url} ã‚¨ãƒ©ãƒ¼: {e}")
        
        return odds_data
    
    def _extract_odds_from_table(self, table, race_id: str) -> Dict[int, Dict]:
        """ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ã‚ªãƒƒã‚ºãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        odds_data = {}
        
        try:
            # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ãƒã‚§ãƒƒã‚¯
            header_row = table.find('tr')
            if not header_row:
                return odds_data
            
            header_cells = header_row.find_all(['th', 'td'])
            header_texts = [cell.get_text(strip=True) for cell in header_cells]
            
            # ã‚ªãƒƒã‚ºãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã©ã†ã‹åˆ¤å®š
            has_odds = any('ã‚ªãƒƒã‚º' in text for text in header_texts)
            has_popularity = any('äººæ°—' in text for text in header_texts)
            
            if not (has_odds or has_popularity):
                return odds_data
            
            # åˆ—ä½ç½®ã‚’ç‰¹å®š
            popularity_col = -1
            umaban_col = -1
            odds_col = -1
            
            for i, text in enumerate(header_texts):
                if 'äººæ°—' in text:
                    popularity_col = i
                elif 'é¦¬ç•ª' in text:
                    umaban_col = i
                elif 'ã‚ªãƒƒã‚º' in text:
                    odds_col = i
            
            # ãƒ‡ãƒ¼ã‚¿è¡Œã‚’è§£æ
            rows = table.find_all('tr')[1:]  # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—
            
            for row in rows:
                cells = row.find_all(['td', 'th'])
                if len(cells) <= max(popularity_col, umaban_col, odds_col):
                    continue
                
                # äººæ°—
                popularity = None
                if popularity_col >= 0 and popularity_col < len(cells):
                    pop_text = cells[popularity_col].get_text(strip=True)
                    if pop_text.isdigit() and 1 <= int(pop_text) <= 18:
                        popularity = int(pop_text)
                
                # é¦¬ç•ª
                umaban = None
                if umaban_col >= 0 and umaban_col < len(cells):
                    umaban_text = cells[umaban_col].get_text(strip=True)
                    if umaban_text.isdigit() and 1 <= int(umaban_text) <= 18:
                        umaban = int(umaban_text)
                
                # ã‚ªãƒƒã‚º
                odds = None
                if odds_col >= 0 and odds_col < len(cells):
                    odds_text = cells[odds_col].get_text(strip=True)
                    if odds_text and odds_text not in ['---.-', '**', '--', '']:
                        try:
                            if re.match(r'^\d+\.\d+$', odds_text):
                                odds = float(odds_text)
                        except:
                            pass
                
                # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
                if popularity and umaban:
                    odds_data[umaban] = {
                        'äººæ°—': popularity,
                        'ã‚ªãƒƒã‚º': odds
                    }
        
        except Exception:
            pass
        
        return odds_data
    
    def _scrape_result_page(self, race_id: str) -> Dict:
        """çµæœãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰"""
        result_data = {}
        
        url = f"{self.base_url}/race/result.html?race_id={race_id}"
        
        try:
            time.sleep(random.uniform(1.0, 2.0))
            
            response = self.session.get(url, timeout=15)
            if response.status_code != 200:
                return result_data
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # çµæœãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã™
            result_table = soup.find('table', class_='race_table_01')
            if result_table:
                print("âœ“ çµæœãƒšãƒ¼ã‚¸ã‚’ç™ºè¦‹ï¼ˆéå»ãƒ¬ãƒ¼ã‚¹ï¼‰")
                # çµæœãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…
                # ã“ã“ã§ã¯ç°¡ç•¥åŒ–
                result_data['has_result'] = True
            
        except Exception as e:
            print(f"âš ï¸ çµæœãƒšãƒ¼ã‚¸ã‚¨ãƒ©ãƒ¼: {e}")
        
        return result_data
    
    def _merge_all_data(self, shutuba_data: pd.DataFrame, 
                       odds_data: Dict[int, Dict], 
                       result_data: Dict, 
                       race_id: str) -> pd.DataFrame:
        """å…¨ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ"""
        if shutuba_data.empty:
            return pd.DataFrame()
        
        final_data = shutuba_data.copy()
        
        # ã‚ªãƒƒã‚ºã¨äººæ°—ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        final_data['ã‚ªãƒƒã‚º'] = None
        final_data['äººæ°—'] = None
        
        # ã‚ªãƒƒã‚ºãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ
        if odds_data:
            for _, row in final_data.iterrows():
                umaban = row['é¦¬ç•ª']
                if umaban in odds_data:
                    idx = final_data[final_data['é¦¬ç•ª'] == umaban].index[0]
                    
                    if odds_data[umaban]['ã‚ªãƒƒã‚º'] is not None:
                        final_data.loc[idx, 'ã‚ªãƒƒã‚º'] = odds_data[umaban]['ã‚ªãƒƒã‚º']
                    if odds_data[umaban]['äººæ°—'] is not None:
                        final_data.loc[idx, 'äººæ°—'] = odds_data[umaban]['äººæ°—']
        
        # çµ±è¨ˆæƒ…å ±
        odds_count = final_data['ã‚ªãƒƒã‚º'].notna().sum()
        pop_count = final_data['äººæ°—'].notna().sum()
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿çµ±åˆçµæœ: ã‚ªãƒƒã‚º{odds_count}é ­ã€äººæ°—{pop_count}é ­")
        
        return final_data


def main():
    """æ”¹è‰¯ç‰ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼å®Ÿè¡Œ"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ”¹è‰¯ç‰ˆnetkeiba.comã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼')
    parser.add_argument('race_id', type=str, help='ãƒ¬ãƒ¼ã‚¹ID (ä¾‹: 202505021211)')
    parser.add_argument('--output', type=str, default='improved_race_data.csv', help='å‡ºåŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«')
    
    args = parser.parse_args()
    
    if not args.race_id.isdigit() or len(args.race_id) != 12:
        print("âŒ ãƒ¬ãƒ¼ã‚¹IDã¯12æ¡ã®æ•°å­—ã§å…¥åŠ›ã—ã¦ãã ã•ã„")
        return
    
    scraper = ImprovedNetkeibaScrapor()
    race_data = scraper.scrape_race_complete(args.race_id)
    
    if race_data.empty:
        print("âŒ ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    # CSVä¿å­˜
    race_data.to_csv(args.output, index=False, encoding='utf-8')
    print(f"\nğŸ’¾ ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {args.output}")
    
    # çµæœè¡¨ç¤º
    print(f"\nğŸ“Š å–å¾—çµæœ: {len(race_data)}é ­")
    print("\nğŸ‡ å‡ºé¦¬è¡¨:")
    for _, horse in race_data.iterrows():
        odds_str = f"{horse['ã‚ªãƒƒã‚º']}å€" if pd.notna(horse['ã‚ªãƒƒã‚º']) else "æœªè¨­å®š"
        pop_str = f"{horse['äººæ°—']}äººæ°—" if pd.notna(horse['äººæ°—']) else "æœªè¨­å®š"
        print(f"  {horse['æ ']}æ {horse['é¦¬ç•ª']:2d}ç•ª {horse['é¦¬å']:15s} "
              f"{horse['é¨æ‰‹']:8s} {horse['å©èˆ']:8s} {horse['é¦¬ä½“é‡']:10s} "
              f"{odds_str:8s} {pop_str}")


if __name__ == "__main__":
    main()