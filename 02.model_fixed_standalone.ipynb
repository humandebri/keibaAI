{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 時系列を考慮した適切なモデル評価（スタンドアロン版）\n",
    "\n",
    "データリークを防ぎ、実際の運用を想定した評価を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリのインポート\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import roc_auc_score, precision_recall_curve\n",
    "    import ast\n",
    "    import os\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "except ImportError as e:\n",
    "    print(f\"エラー: {e}\")\n",
    "    print(\"\\n必要なパッケージをインストールしてください:\")\n",
    "    print(\"pip install lightgbm pandas numpy scikit-learn matplotlib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# インラインで評価クラスを定義\n",
    "class ProperModelEvaluator:\n",
    "    \"\"\"時系列を考慮した適切なモデル評価クラス\"\"\"\n",
    "    \n",
    "    def __init__(self, base_dir: str = '.'):\n",
    "        self.base_dir = base_dir\n",
    "        \n",
    "    def load_yearly_data(self, years):\n",
    "        \"\"\"年単位でデータを読み込む\"\"\"\n",
    "        dfs = []\n",
    "        for year in years:\n",
    "            # CSVファイルのパスを試す\n",
    "            file_paths = [\n",
    "                f'{self.base_dir}/encoded/{year}encoded_data.csv',\n",
    "                f'{self.base_dir}/encoded/{year}_encoded_data.csv',\n",
    "                f'{self.base_dir}/encoded/encoded_data_{year}.csv'\n",
    "            ]\n",
    "            \n",
    "            loaded = False\n",
    "            for file_path in file_paths:\n",
    "                if os.path.exists(file_path):\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    print(f\"Loaded {year} data from {file_path}: {len(df)} rows\")\n",
    "                    dfs.append(df)\n",
    "                    loaded = True\n",
    "                    break\n",
    "            \n",
    "            if not loaded:\n",
    "                print(f\"Warning: No data file found for {year}\")\n",
    "        \n",
    "        if not dfs:\n",
    "            # 複数年のデータが1つのファイルにある場合\n",
    "            combined_paths = [\n",
    "                f'{self.base_dir}/encoded/encoded_data.csv',\n",
    "                f'{self.base_dir}/encoded/2022_2023encoded_data.csv'\n",
    "            ]\n",
    "            \n",
    "            for path in combined_paths:\n",
    "                if os.path.exists(path):\n",
    "                    print(f\"Found combined data at {path}\")\n",
    "                    df = pd.read_csv(path)\n",
    "                    \n",
    "                    # 日付から年を抽出してフィルタリング\n",
    "                    if '日付' in df.columns:\n",
    "                        df['year'] = pd.to_datetime(df['日付']).dt.year\n",
    "                        for year in years:\n",
    "                            year_df = df[df['year'] == year].copy()\n",
    "                            if len(year_df) > 0:\n",
    "                                print(f\"Extracted {year} data: {len(year_df)} rows\")\n",
    "                                dfs.append(year_df)\n",
    "                    break\n",
    "            \n",
    "        if not dfs:\n",
    "            raise ValueError(\"No data files found\")\n",
    "            \n",
    "        return pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    def prepare_features(self, df):\n",
    "        \"\"\"特徴量の準備\"\"\"\n",
    "        # 着順を二値分類用に変換（3着以内を1）\n",
    "        df['target'] = df['着順'].map(lambda x: 1 if x < 4 else 0)\n",
    "        \n",
    "        # 不要なカラムを除外\n",
    "        exclude_cols = ['着順', 'target', 'オッズ', '人気', '上がり', '走破時間', '通過順', 'year']\n",
    "        feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "        \n",
    "        return df, feature_cols\n",
    "    \n",
    "    def train_model(self, X_train, y_train, X_valid, y_valid):\n",
    "        \"\"\"モデルの学習\"\"\"\n",
    "        # クラス比率の計算\n",
    "        ratio = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "        \n",
    "        print(f\"\\nTraining set:\")\n",
    "        print(f\"Negative/Positive ratio: {ratio:.2f}\")\n",
    "        print(f\"負例数（3着外）: {(y_train == 0).sum()}\")\n",
    "        print(f\"正例数（3着内）: {(y_train == 1).sum()}\")\n",
    "        \n",
    "        # パラメータ設定\n",
    "        params = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'binary_logloss',\n",
    "            'verbosity': -1,\n",
    "            'boosting_type': 'gbdt',\n",
    "            'scale_pos_weight': ratio,\n",
    "            'random_state': 42,\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.9,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'min_child_samples': 20,\n",
    "            'n_estimators': 300\n",
    "        }\n",
    "        \n",
    "        # モデル学習\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # 検証データでの予測\n",
    "        y_pred = model.predict_proba(X_valid)[:, 1]\n",
    "        auc_score = roc_auc_score(y_valid, y_pred)\n",
    "        \n",
    "        # 最適閾値の探索\n",
    "        precisions, recalls, thresholds = precision_recall_curve(y_valid, y_pred)\n",
    "        fbeta_scores = (1 + 0.5**2) * (precisions * recalls) / (0.5**2 * precisions + recalls)\n",
    "        best_idx = np.argmax(fbeta_scores[:-1])\n",
    "        optimal_threshold = thresholds[best_idx]\n",
    "        \n",
    "        print(f\"\\nValidation AUC: {auc_score:.4f}\")\n",
    "        print(f\"Optimal threshold: {optimal_threshold:.4f}\")\n",
    "        \n",
    "        return model, optimal_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 単純な時系列分割での評価\n",
    "\n",
    "まず、既存のデータを正しく時系列で分割して評価します。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# モデルの学習（時系列分割版）\nif 'X_train' in locals():\n    evaluator = ProperModelEvaluator()\n    model, threshold = evaluator.train_model(X_train, y_train, X_valid, y_valid)\n    \n    # テストデータでの評価\n    y_pred_test = model.predict_proba(X_test)[:, 1]\n    test_auc = roc_auc_score(y_test, y_pred_test)\n    \n    # 混同行列\n    TP = ((y_test == 1) & (y_pred_test >= threshold)).sum()\n    FP = ((y_test == 0) & (y_pred_test >= threshold)).sum()\n    TN = ((y_test == 0) & (y_pred_test < threshold)).sum()\n    FN = ((y_test == 1) & (y_pred_test < threshold)).sum()\n    \n    total = len(y_test)\n    \n    print(f\"\\n=== テストセットの結果（時系列分割） ===\")\n    print(f\"テストAUC: {test_auc:.4f}\")\n    print(f\"総レース数: {total}件\")\n    print(f\"正解（3着内を的中）:  {TP}件 ({TP/total*100:.2f}%)\")\n    print(f\"誤検出（3着外を3着内と予測）: {FP}件 ({FP/total*100:.2f}%)\")\n    print(f\"正解（3着外を的中）:  {TN}件 ({TN/total*100:.2f}%)\")\n    print(f\"見逃し（3着内を3着外と予測）: {FN}件 ({FN/total*100:.2f}%)\")\n    \n    # 予測精度の計算\n    if TP + FP > 0:\n        precision = TP / (TP + FP)\n        print(f\"\\n予測精度（Precision）: {precision:.2%}\")\n        print(f\"→ 3着内と予測した馬のうち、実際に3着内だった割合\")\n    \n    if TP + FN > 0:\n        recall = TP / (TP + FN)\n        print(f\"再現率（Recall）: {recall:.2%}\")\n        print(f\"→ 実際の3着内の馬のうち、予測できた割合\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 回収率計算関数\ndef calculate_returns_time_series(test_data, predictions, threshold):\n    \"\"\"時系列分割したテストデータでの回収率計算\"\"\"\n    \n    # テストデータの年を特定\n    test_years = pd.to_datetime(test_data['日付']).dt.year.unique()\n    print(f\"テストデータの年: {sorted(test_years)}\")\n    \n    # 払戻データを読み込み\n    payback_dfs = []\n    for year in test_years:\n        payback_path = f'payback/{year}.csv'\n        if os.path.exists(payback_path):\n            df = pd.read_csv(payback_path, encoding='SHIFT-JIS', dtype={'race_id': str})\n            df['race_id'] = df['race_id'].str.replace(r'\\.0$', '', regex=True)\n            payback_dfs.append(df)\n            print(f\"{year}年の払戻データを読み込みました\")\n        else:\n            print(f\"警告: {year}年の払戻データが見つかりません\")\n    \n    if not payback_dfs:\n        print(\"払戻データが見つかりません\")\n        return None\n    \n    payback_df = pd.concat(payback_dfs, ignore_index=True)\n    payback_df.set_index('race_id', inplace=True)\n    \n    # 払戻データの変換\n    for col in ['単勝', '複勝']:\n        if col in payback_df.columns:\n            payback_df[col] = payback_df[col].apply(\n                lambda x: ast.literal_eval(x) if pd.notna(x) and str(x).strip().startswith('[') else []\n            )\n    \n    # 賭ける馬の決定\n    betting_horses = []\n    for i in range(len(predictions)):\n        if predictions[i] >= threshold:\n            race_id = str(int(float(test_data.iloc[i]['race_id'])))\n            horse_num = str(int(float(test_data.iloc[i]['馬番'])))\n            betting_horses.append((race_id, horse_num))\n    \n    print(f\"\\n賭ける馬数: {len(betting_horses)}頭\")\n    \n    # 回収金額の計算\n    win_return = 0\n    place_return = 0\n    \n    for race_id, horse_num in betting_horses:\n        if race_id in payback_df.index:\n            race_data = payback_df.loc[race_id]\n            \n            # 単勝\n            if '単勝' in race_data and isinstance(race_data['単勝'], list):\n                win_data = race_data['単勝']\n                for j in range(0, len(win_data), 2):\n                    if j+1 < len(win_data) and win_data[j] == horse_num:\n                        win_return += int(win_data[j + 1].replace(',', ''))\n            \n            # 複勝\n            if '複勝' in race_data and isinstance(race_data['複勝'], list):\n                place_data = race_data['複勝']\n                for j in range(0, len(place_data), 2):\n                    if j+1 < len(place_data) and place_data[j] == horse_num:\n                        place_return += int(place_data[j + 1].replace(',', ''))\n    \n    # 回収率計算\n    bet_amount = len(betting_horses) * 100\n    win_return_rate = (win_return / bet_amount * 100) if bet_amount > 0 else 0\n    place_return_rate = (place_return / bet_amount * 100) if bet_amount > 0 else 0\n    \n    print(f\"\\n=== 回収率結果（時系列分割） ===\")\n    print(f\"賭けた回数: {len(betting_horses)}回\")\n    print(f\"投資金額: {bet_amount:,}円\")\n    print(f\"単勝払戻: {win_return:,}円\")\n    print(f\"複勝払戻: {place_return:,}円\")\n    print(f\"単勝回収率: {win_return_rate:.2f}%\")\n    print(f\"複勝回収率: {place_return_rate:.2f}%\")\n    \n    return {\n        'win_return_rate': win_return_rate,\n        'place_return_rate': place_return_rate,\n        'bet_count': len(betting_horses)\n    }\n\n# 回収率の計算\nif 'y_pred_test' in locals() and 'test_data' in locals():\n    returns = calculate_returns_time_series(test_data, y_pred_test, threshold)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# オリジナルの方法との比較\nprint(\"\\n=== オリジナル手法との比較 ===\")\nprint(\"\\nオリジナル（データリークあり）:\")\nprint(\"- 学習/テスト分割: ランダムに70/30分割\")\nprint(\"- 学習データに未来のデータが混入\")\nprint(\"- 報告された単勝回収率: 130.84%\")\nprint(\"- 報告された複勝回収率: 132.84%\")\nprint(\"\\n時系列を考慮した正しい分割:\")\nprint(\"- 学習データ: 過去の日付のみ\")\nprint(\"- テストデータ: 未来の日付のみ\")\nprint(\"- データリークなし\")\nif 'returns' in locals() and returns:\n    print(f\"- 実際の単勝回収率: {returns['win_return_rate']:.2f}%\")\n    print(f\"- 実際の複勝回収率: {returns['place_return_rate']:.2f}%\")\n    print(\"\\n【結論】データリークの影響で回収率が大幅に過大評価されていました！\")\n    \n    # 回収率の評価\n    print(\"\\n=== 回収率の評価 ===\")\n    if returns['win_return_rate'] >= 100:\n        print(\"✅ 単勝: 黒字（100%以上）\")\n    else:\n        print(f\"❌ 単勝: 赤字（{100 - returns['win_return_rate']:.1f}%の損失）\")\n    \n    if returns['place_return_rate'] >= 100:\n        print(\"✅ 複勝: 黒字（100%以上）\")\n    else:\n        print(f\"❌ 複勝: 赤字（{100 - returns['place_return_rate']:.1f}%の損失）\")\n        \n    print(\"\\n【現実的な競馬予測の難しさ】\")\n    print(\"- 競馬の控除率（約20-30%）を考慮すると、長期的に100%超えは非常に困難\")\n    print(\"- 70-90%の回収率でも、予測モデルとしては優秀\")\n    print(\"- データリークなしでの正確な評価が重要\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 特徴量重要度を表示\nif 'model' in locals():\n    importance = model.feature_importances_\n    feature_names = X_train.columns\n    indices = np.argsort(importance)[::-1]\n    \n    print(\"\\n=== 重要な特徴量トップ20 ===\")\n    for i in range(min(20, len(indices))):\n        idx = indices[i]\n        print(f\"{i+1:2d}) {feature_names[idx]:<30} 重要度: {importance[idx]:>6}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. オリジナルの方法との比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Comparison with Original Method ===\n",
      "\n",
      "Original (with data leakage):\n",
      "- Train/Test split: Random 70/30 split\n",
      "- Uses future data in training\n",
      "- Reported win return: 130.84%\n",
      "- Reported place return: 132.84%\n",
      "\n",
      "Proper Time Series Split:\n",
      "- Train: Earlier dates only\n",
      "- Test: Later dates only\n",
      "- No future data leakage\n"
     ]
    }
   ],
   "source": [
    "# オリジナルの方法（データリークあり）での結果を表示\n",
    "print(\"\\n=== Comparison with Original Method ===\")\n",
    "print(\"\\nOriginal (with data leakage):\")\n",
    "print(\"- Train/Test split: Random 70/30 split\")\n",
    "print(\"- Uses future data in training\")\n",
    "print(\"- Reported win return: 130.84%\")\n",
    "print(\"- Reported place return: 132.84%\")\n",
    "print(\"\\nProper Time Series Split:\")\n",
    "print(\"- Train: Earlier dates only\")\n",
    "print(\"- Test: Later dates only\")\n",
    "print(\"- No future data leakage\")\n",
    "if 'returns' in locals() and returns:\n",
    "    print(f\"- Actual win return: {returns['win_return_rate']:.2f}%\")\n",
    "    print(f\"- Actual place return: {returns['place_return_rate']:.2f}%\")\n",
    "    print(\"\\nThe difference shows the impact of data leakage!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 特徴量重要度の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量重要度を表示\n",
    "if 'model' in locals():\n",
    "    importance = model.feature_importances_\n",
    "    feature_names = X_train.columns\n",
    "    indices = np.argsort(importance)[::-1]\n",
    "    \n",
    "    print(\"\\nTop 20 Important Features:\")\n",
    "    for i in range(min(20, len(indices))):\n",
    "        idx = indices[i]\n",
    "        print(f\"{i+1:2d}) {feature_names[idx]:<30} {importance[idx]:>6}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}