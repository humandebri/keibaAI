#!/usr/bin/env python3
"""
Clean ML Model Training
ã‚ªãƒƒã‚ºã‚’ä½¿ã‚ãªã„çœŸã®æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã‚·ã‚¹ãƒ†ãƒ 
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score, accuracy_score, classification_report
import joblib
import warnings
import os
from datetime import datetime

warnings.filterwarnings('ignore')


class CleanModelTrainer:
    """ã‚¯ãƒªãƒ¼ãƒ³ãªæ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.model = None
        self.scaler = StandardScaler()
        self.feature_columns = None
        self.feature_importance = None
        self.model_metrics = {}
        self.jockey_stats = {}
        self.trainer_stats = {}
        
    def load_training_data(self, data_path="encoded/2020_2025encoded_data_v2.csv"):
        """è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        print("ğŸ“Š è¨“ç·´ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹")
        
        try:
            df = pd.read_csv(data_path)
            print(f"   ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df):,}ä»¶")
            print(f"   å‹åˆ©ã‚±ãƒ¼ã‚¹: {(df['ç€é †'] == 1).sum():,}ä»¶ ({(df['ç€é †'] == 1).mean():.3f})")
            print(f"   ç‰¹å¾´é‡æ•°: {len(df.columns)}å€‹")
            
            return df
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def create_clean_features(self, df: pd.DataFrame) -> tuple:
        """ã‚ªãƒƒã‚ºã‚’ä½¿ã‚ãªã„ã‚¯ãƒªãƒ¼ãƒ³ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"""
        print("ğŸ”§ ã‚¯ãƒªãƒ¼ãƒ³ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°é–‹å§‹")
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°
        y = (df['ç€é †'] == 1).astype(int)
        
        # åŸºæœ¬ç‰¹å¾´é‡ï¼ˆã‚ªãƒƒã‚ºé™¤å¤–ï¼‰
        basic_features = [
            'äººæ°—', 'ä½“é‡', 'ä½“é‡å¤‰åŒ–', 'æ–¤é‡', 'ä¸ŠãŒã‚Š', 
            'å‡ºèµ°é ­æ•°', 'è·é›¢', 'ã‚¯ãƒ©ã‚¹', 'é¨æ‰‹ã®å‹ç‡', 'æ€§', 'é½¢'
        ]
        
        # éå»æˆç¸¾ï¼ˆã‚ªãƒƒã‚ºé™¤å¤–ï¼‰
        past_features = []
        for i in range(1, 6):
            past_features.extend([f'ç€é †{i}', f'è·é›¢{i}', f'é€šéé †{i}', f'èµ°ç ´æ™‚é–“{i}'])
        
        # æ™‚ç³»åˆ—ãƒ»çµ±è¨ˆãƒ»ãƒ¬ãƒ¼ã‚¹æ¡ä»¶
        temporal_features = ['æ—¥ä»˜å·®1', 'æ—¥ä»˜å·®2', 'æ—¥ä»˜å·®3']
        stat_features = ['å¹³å‡ã‚¹ãƒ”ãƒ¼ãƒ‰', 'éå»5èµ°ã®åˆè¨ˆè³é‡‘', 'å¹³å‡æ–¤é‡']
        race_features = ['èŠãƒ»ãƒ€ãƒ¼ãƒˆ', 'å›ã‚Š', 'é¦¬å ´', 'å¤©æ°—', 'å ´id']
        
        # ä½¿ç”¨å¯èƒ½ç‰¹å¾´é‡
        all_candidates = basic_features + past_features + temporal_features + stat_features + race_features
        available_features = [col for col in all_candidates if col in df.columns]
        
        enhanced_df = df.copy()
        selected_features = available_features.copy()
        
        # é¦¬ã®éå»æˆç¸¾åˆ†æ
        if all(f'ç€é †{i}' in df.columns for i in range(1, 6)):
            past_positions = [df[f'ç€é †{i}'].fillna(10) for i in range(1, 6)]
            if past_positions:
                past_pos_df = pd.concat(past_positions, axis=1)
                enhanced_df['éå»å¹³å‡ç€é †'] = past_pos_df.mean(axis=1)
                enhanced_df['éå»æœ€é«˜ç€é †'] = past_pos_df.min(axis=1)
                enhanced_df['å‹åˆ©çµŒé¨“'] = (past_pos_df == 1).sum(axis=1)
                enhanced_df['è¤‡å‹çµŒé¨“'] = (past_pos_df <= 3).sum(axis=1)
                enhanced_df['ç€é †å®‰å®šæ€§'] = past_pos_df.std(axis=1).fillna(5)
                selected_features.extend(['éå»å¹³å‡ç€é †', 'éå»æœ€é«˜ç€é †', 'å‹åˆ©çµŒé¨“', 'è¤‡å‹çµŒé¨“', 'ç€é †å®‰å®šæ€§'])
        
        # ä¼‘é¤ŠæœŸé–“åˆ†æ
        if 'æ—¥ä»˜å·®1' in df.columns:
            enhanced_df['ä¼‘é¤ŠæœŸé–“'] = df['æ—¥ä»˜å·®1'].fillna(30)
            enhanced_df['ä¼‘é¤Šé©æ­£'] = np.where(
                (enhanced_df['ä¼‘é¤ŠæœŸé–“'] >= 14) & (enhanced_df['ä¼‘é¤ŠæœŸé–“'] <= 60), 1.2,
                np.where(enhanced_df['ä¼‘é¤ŠæœŸé–“'] < 14, 0.9, 0.8)
            )
            selected_features.extend(['ä¼‘é¤ŠæœŸé–“', 'ä¼‘é¤Šé©æ­£'])
        
        # è·é›¢é©æ€§åˆ†æ
        if 'è·é›¢' in df.columns:
            current_distance = df['è·é›¢'].fillna(1600)
            enhanced_df['è·é›¢ã‚«ãƒ†ã‚´ãƒª'] = pd.cut(current_distance, 
                                        bins=[0, 1400, 1800, 2200, 3000], 
                                        labels=[1, 2, 3, 4]).astype(float)
            selected_features.append('è·é›¢ã‚«ãƒ†ã‚´ãƒª')
        
        # é¨æ‰‹ãƒ»èª¿æ•™å¸«è©³ç´°æˆç¸¾
        if 'é¨æ‰‹' in df.columns:
            print("   ğŸ‡ é¨æ‰‹å®Ÿç¸¾è¨ˆç®—ä¸­...")
            jockey_stats = df.groupby('é¨æ‰‹').agg({
                'ç€é †': ['count', lambda x: (x == 1).sum(), lambda x: (x <= 3).sum()]
            }).round(4)
            jockey_stats.columns = ['é¨ä¹—æ•°', 'å‹åˆ©æ•°', 'è¤‡å‹æ•°']
            jockey_stats['å®Ÿå‹ç‡'] = jockey_stats['å‹åˆ©æ•°'] / jockey_stats['é¨ä¹—æ•°']
            jockey_stats['å®Ÿè¤‡å‹ç‡'] = jockey_stats['è¤‡å‹æ•°'] / jockey_stats['é¨ä¹—æ•°']
            
            self.jockey_stats = {
                'å®Ÿå‹ç‡': dict(zip(jockey_stats.index, jockey_stats['å®Ÿå‹ç‡'])),
                'å®Ÿè¤‡å‹ç‡': dict(zip(jockey_stats.index, jockey_stats['å®Ÿè¤‡å‹ç‡'])),
                'é¨ä¹—æ•°': dict(zip(jockey_stats.index, jockey_stats['é¨ä¹—æ•°']))
            }
            
            enhanced_df['é¨æ‰‹å®Ÿå‹ç‡'] = df['é¨æ‰‹'].map(jockey_stats['å®Ÿå‹ç‡']).fillna(0.08)
            enhanced_df['é¨æ‰‹å®Ÿè¤‡å‹ç‡'] = df['é¨æ‰‹'].map(jockey_stats['å®Ÿè¤‡å‹ç‡']).fillna(0.25)
            selected_features.extend(['é¨æ‰‹å®Ÿå‹ç‡', 'é¨æ‰‹å®Ÿè¤‡å‹ç‡'])
        
        if 'èª¿æ•™å¸«' in df.columns:
            print("   ğŸ‘” èª¿æ•™å¸«å®Ÿç¸¾è¨ˆç®—ä¸­...")
            trainer_stats = df.groupby('èª¿æ•™å¸«').agg({
                'ç€é †': ['count', lambda x: (x == 1).sum()]
            }).round(4)
            trainer_stats.columns = ['ç®¡ç†æ•°', 'å‹åˆ©æ•°']
            trainer_stats['å®Ÿå‹ç‡'] = trainer_stats['å‹åˆ©æ•°'] / trainer_stats['ç®¡ç†æ•°']
            
            self.trainer_stats = {
                'å®Ÿå‹ç‡': dict(zip(trainer_stats.index, trainer_stats['å®Ÿå‹ç‡'])),
                'ç®¡ç†æ•°': dict(zip(trainer_stats.index, trainer_stats['ç®¡ç†æ•°']))
            }
            
            enhanced_df['èª¿æ•™å¸«å®Ÿå‹ç‡'] = df['èª¿æ•™å¸«'].map(trainer_stats['å®Ÿå‹ç‡']).fillna(0.06)
            selected_features.append('èª¿æ•™å¸«å®Ÿå‹ç‡')
        
        # æ ç•ªåˆ†æ
        if 'æ ' in df.columns:
            enhanced_df['æ ç•ª'] = df['æ '].fillna(4)
            enhanced_df['å†…æ '] = (enhanced_df['æ ç•ª'] <= 3).astype(int)
            enhanced_df['å¤–æ '] = (enhanced_df['æ ç•ª'] >= 7).astype(int)
            selected_features.extend(['æ ç•ª', 'å†…æ ', 'å¤–æ '])
        
        # äººæ°—ã®ã¿ã®å¸‚å ´è©•ä¾¡
        if 'äººæ°—' in df.columns:
            enhanced_df['æœ¬å‘½'] = (enhanced_df['äººæ°—'] <= 3).astype(int)
            enhanced_df['å¤§ç©´'] = (enhanced_df['äººæ°—'] >= 9).astype(int)
            enhanced_df['äººæ°—é€†æ•°'] = 1.0 / enhanced_df['äººæ°—'].fillna(9)
            selected_features.extend(['æœ¬å‘½', 'å¤§ç©´', 'äººæ°—é€†æ•°'])
        
        # æœ€çµ‚ç‰¹å¾´é‡ãƒãƒˆãƒªãƒƒã‚¯ã‚¹
        final_features = [col for col in selected_features if col in enhanced_df.columns]
        X = enhanced_df[final_features].copy()
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        X = X.replace([np.inf, -np.inf], 0)
        X = X.fillna(0)
        X = X.select_dtypes(include=[np.number])
        
        self.feature_columns = list(X.columns)
        
        print(f"âœ… ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Œäº†")
        print(f"   æœ€çµ‚ç‰¹å¾´é‡æ•°: {len(self.feature_columns)}å€‹")
        print(f"   ã‚ªãƒƒã‚ºé–¢é€£ç‰¹å¾´é‡: é™¤å¤–æ¸ˆã¿")
        
        return X, y
    
    def train_model(self, X: pd.DataFrame, y: pd.Series, optimize=True):
        """ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
        print("ğŸ¤– ã‚¯ãƒªãƒ¼ãƒ³ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹")
        
        # åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.15, random_state=42, stratify=y
        )
        
        print(f"   è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {X_train.shape[0]:,}ä»¶")
        print(f"   æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {X_test.shape[0]:,}ä»¶")
        print(f"   ç‰¹å¾´é‡æ•°: {X_train.shape[1]}å€‹")
        print(f"   æ­£ä¾‹ã®å‰²åˆ: {y_train.mean():.3f}")
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        if optimize:
            # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
            print("   ğŸ” ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ä¸­...")
            param_grid = {
                'n_estimators': [200, 300, 400],
                'max_depth': [15, 20, 25],
                'min_samples_split': [50, 80, 100],
                'min_samples_leaf': [25, 40, 50],
                'max_features': ['sqrt', 'log2']
            }
            
            rf = RandomForestClassifier(
                random_state=42,
                class_weight='balanced',
                n_jobs=-1,
                oob_score=True
            )
            
            grid_search = GridSearchCV(
                rf, param_grid, cv=3, scoring='roc_auc', n_jobs=-1, verbose=0
            )
            grid_search.fit(X_train_scaled, y_train)
            
            self.model = grid_search.best_estimator_
            print(f"   æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {grid_search.best_params_}")
            print(f"   CV AUC: {grid_search.best_score_:.3f}")
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            self.model = RandomForestClassifier(
                n_estimators=300,
                max_depth=20,
                min_samples_split=80,
                min_samples_leaf=40,
                max_features='sqrt',
                random_state=42,
                class_weight='balanced',
                n_jobs=-1,
                oob_score=True
            )
            self.model.fit(X_train_scaled, y_train)
        
        # è©•ä¾¡
        test_pred = self.model.predict(X_test_scaled)
        test_proba = self.model.predict_proba(X_test_scaled)[:, 1]
        
        test_accuracy = accuracy_score(y_test, test_pred)
        test_auc = roc_auc_score(y_test, test_proba)
        oob_score = self.model.oob_score_
        
        self.model_metrics = {
            'model_name': 'CleanRandomForest',
            'test_accuracy': test_accuracy,
            'test_auc': test_auc,
            'oob_score': oob_score,
            'feature_count': len(self.feature_columns),
            'training_samples': len(X_train)
        }
        
        # ç‰¹å¾´é‡é‡è¦åº¦
        self.feature_importance = pd.DataFrame({
            'feature': self.feature_columns,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†")
        print(f"   æ¤œè¨¼ç²¾åº¦: {test_accuracy:.3f}")
        print(f"   æ¤œè¨¼AUC: {test_auc:.3f}")
        print(f"   OOBç²¾åº¦: {oob_score:.3f}")
        
        print(f"\nğŸ“Š ç‰¹å¾´é‡é‡è¦åº¦ Top 10:")
        for _, row in self.feature_importance.head(10).iterrows():
            print(f"      {row['feature']}: {row['importance']:.4f}")
        
        return self.model
    
    def save_model(self, model_dir="models"):
        """ãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
        os.makedirs(model_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'feature_columns': self.feature_columns,
            'feature_importance': self.feature_importance,
            'model_metrics': self.model_metrics,
            'jockey_stats': self.jockey_stats,
            'trainer_stats': self.trainer_stats
        }
        
        filepath = f"{model_dir}/clean_model_{timestamp}.pkl"
        joblib.dump(model_data, filepath)
        
        # æœ€æ–°ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã‚‚ä¿å­˜
        latest_filepath = f"{model_dir}/clean_model_latest.pkl"
        joblib.dump(model_data, latest_filepath)
        
        print(f"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†:")
        print(f"   {filepath}")
        print(f"   {latest_filepath}")
        
        return filepath
    
    def load_model(self, filepath):
        """ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""
        print(f"ğŸ“‚ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {filepath}")
        
        try:
            model_data = joblib.load(filepath)
            
            self.model = model_data['model']
            self.scaler = model_data['scaler']
            self.feature_columns = model_data['feature_columns']
            self.feature_importance = model_data['feature_importance']
            self.model_metrics = model_data['model_metrics']
            self.jockey_stats = model_data.get('jockey_stats', {})
            self.trainer_stats = model_data.get('trainer_stats', {})
            
            print(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
            print(f"   AUC: {self.model_metrics.get('test_auc', 'N/A')}")
            print(f"   ç‰¹å¾´é‡æ•°: {len(self.feature_columns)}å€‹")
            
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def run_training_pipeline(self, data_path="encoded/2020_2025encoded_data_v2.csv", optimize=True):
        """å®Œå…¨ãªè¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        print("ğŸš€ ã‚¯ãƒªãƒ¼ãƒ³æ©Ÿæ¢°å­¦ç¿’è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹")
        print("ğŸ’¡ ã‚ªãƒƒã‚ºã‚’ä½¿ã‚ãªã„çœŸã®æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«")
        
        # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        df = self.load_training_data(data_path)
        if df is None:
            return None
        
        # 2. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
        X, y = self.create_clean_features(df)
        if X is None:
            return None
        
        # 3. ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        model = self.train_model(X, y, optimize=optimize)
        if model is None:
            return None
        
        # 4. ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        model_path = self.save_model()
        
        print(f"\nğŸ“Š è¨“ç·´å®Œäº†ã‚µãƒãƒªãƒ¼:")
        print(f"   ãƒ¢ãƒ‡ãƒ«: {self.model_metrics['model_name']}")
        print(f"   AUC: {self.model_metrics['test_auc']:.3f}")
        print(f"   ç²¾åº¦: {self.model_metrics['test_accuracy']:.3f}")
        print(f"   OOBç²¾åº¦: {self.model_metrics['oob_score']:.3f}")
        print(f"   ç‰¹å¾´é‡æ•°: {self.model_metrics['feature_count']}å€‹")
        print(f"   è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«: {self.model_metrics['training_samples']:,}ä»¶")
        print(f"   âš¡ ã‚ªãƒƒã‚ºéä¾å­˜ã®çœŸã®æ©Ÿæ¢°å­¦ç¿’")
        
        print(f"\nâœ… ã‚¯ãƒªãƒ¼ãƒ³æ©Ÿæ¢°å­¦ç¿’è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†")
        return model_path


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    trainer = CleanModelTrainer()
    model_path = trainer.run_training_pipeline(optimize=False)  # é«˜é€Ÿå®Ÿè¡Œ
    
    if model_path:
        print(f"\nğŸ‰ è¨“ç·´ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«: {model_path}")


if __name__ == "__main__":
    main()