#!/usr/bin/env python3
"""
netkeiba.com ã‚ªãƒƒã‚ºç‰¹åŒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ï¼ˆSeleniumä¸ä½¿ç”¨ï¼‰
ã‚ªãƒƒã‚ºã¨äººæ°—ã‚’ç¢ºå®Ÿã«å–å¾—ã™ã‚‹å°‚ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import time
import random
import json
from typing import Dict, List, Optional


class OddsFocusedScraper:
    """ã‚ªãƒƒã‚ºå–å¾—ã«ç‰¹åŒ–ã—ãŸã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ï¼ˆSeleniumä¸ä½¿ç”¨ï¼‰"""
    
    PLACE_DICT = {
        "01": "æœ­å¹Œ", "02": "å‡½é¤¨", "03": "ç¦å³¶", "04": "æ–°æ½Ÿ", "05": "æ±äº¬",
        "06": "ä¸­å±±", "07": "ä¸­äº¬", "08": "äº¬éƒ½", "09": "é˜ªç¥", "10": "å°å€‰"
    }
    
    USER_AGENTS = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/91.0.864.67",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36",
        "Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1"
    ]
    
    def __init__(self):
        self.session = requests.Session()
        self._setup_session()
        
    def _setup_session(self):
        """é«˜åº¦ãªã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®š"""
        user_agent = random.choice(self.USER_AGENTS)
        self.session.headers.update({
            'User-Agent': user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'max-age=0',
            'Referer': 'https://race.netkeiba.com/',
        })
        
        # Cookieã®åˆæœŸè¨­å®š
        self.session.get('https://race.netkeiba.com/', timeout=10)
    
    def scrape_race_with_odds(self, race_id: str) -> pd.DataFrame:
        """ã‚ªãƒƒã‚ºã‚’å«ã‚€å®Œå…¨ãªãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        print(f"ğŸ‡ ãƒ¬ãƒ¼ã‚¹æƒ…å ±å–å¾—ä¸­: {race_id}")
        
        # æ‰‹æ³•1: ã‚ªãƒƒã‚ºå°‚ç”¨ãƒšãƒ¼ã‚¸ã‹ã‚‰ã®å–å¾—
        odds_data = self._get_odds_from_odds_page(race_id)
        
        # æ‰‹æ³•2: å‡ºé¦¬è¡¨ãƒšãƒ¼ã‚¸ã‹ã‚‰ã®å–å¾—
        shutuba_data = self._get_data_from_shutuba_page(race_id)
        
        # æ‰‹æ³•3: ãƒ¢ãƒã‚¤ãƒ«ç‰ˆã‹ã‚‰ã®å–å¾—ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãªæ§‹é€ ï¼‰
        mobile_data = self._get_data_from_mobile_page(race_id)
        
        # æ‰‹æ³•4: Ajax/API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ã®å–å¾—
        api_data = self._get_data_from_api(race_id)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ
        final_data = self._merge_all_data(odds_data, shutuba_data, mobile_data, api_data, race_id)
        
        if final_data.empty:
            print("âŒ å…¨ã¦ã®æ‰‹æ³•ã§ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—")
            return pd.DataFrame()
        
        print(f"âœ… {len(final_data)}é ­ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—")
        return final_data
    
    def _get_odds_from_odds_page(self, race_id: str) -> Optional[pd.DataFrame]:
        """ã‚ªãƒƒã‚ºå°‚ç”¨ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        print("ğŸ“Š ã‚ªãƒƒã‚ºãƒšãƒ¼ã‚¸ã‹ã‚‰ã®å–å¾—ã‚’è©¦è¡Œ...")
        
        odds_urls = [
            f"https://race.netkeiba.com/odds/index.html?race_id={race_id}",
            f"https://race.netkeiba.com/odds/win.html?race_id={race_id}",
            f"https://race.netkeiba.com/api/api_get_jockey_result.html?race_id={race_id}",
        ]
        
        for url in odds_urls:
            try:
                time.sleep(random.uniform(1.0, 2.0))  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
                
                response = self.session.get(url, timeout=15)
                if response.status_code == 200:
                    print(f"âœ“ ã‚ªãƒƒã‚ºãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹æˆåŠŸ: {url}")
                    
                    # JSON ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å ´åˆ
                    if 'application/json' in response.headers.get('content-type', ''):
                        try:
                            data = response.json()
                            parsed = self._parse_odds_json(data, race_id)
                            if parsed is not None:
                                return parsed
                        except:
                            pass
                    
                    # HTML ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å ´åˆ
                    soup = BeautifulSoup(response.content, 'html.parser')
                    parsed = self._parse_odds_html(soup, race_id)
                    if parsed is not None:
                        return parsed
                        
            except Exception as e:
                print(f"âš ï¸ ã‚ªãƒƒã‚ºãƒšãƒ¼ã‚¸ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        return None
    
    def _get_data_from_shutuba_page(self, race_id: str) -> Optional[pd.DataFrame]:
        """å‡ºé¦¬è¡¨ãƒšãƒ¼ã‚¸ã‹ã‚‰åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        print("ğŸ“‹ å‡ºé¦¬è¡¨ãƒšãƒ¼ã‚¸ã‹ã‚‰ã®å–å¾—ã‚’è©¦è¡Œ...")
        
        url = f"https://race.netkeiba.com/race/shutuba.html?race_id={race_id}"
        
        try:
            time.sleep(random.uniform(0.5, 1.5))
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            return self._parse_shutuba_page(soup, race_id)
            
        except Exception as e:
            print(f"âš ï¸ å‡ºé¦¬è¡¨ãƒšãƒ¼ã‚¸ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _get_data_from_mobile_page(self, race_id: str) -> Optional[pd.DataFrame]:
        """ãƒ¢ãƒã‚¤ãƒ«ç‰ˆã‹ã‚‰ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        print("ğŸ“± ãƒ¢ãƒã‚¤ãƒ«ç‰ˆã‹ã‚‰ã®å–å¾—ã‚’è©¦è¡Œ...")
        
        # ãƒ¢ãƒã‚¤ãƒ«ç”¨User-Agentã«å¤‰æ›´
        mobile_headers = self.session.headers.copy()
        mobile_headers['User-Agent'] = "Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15"
        
        mobile_urls = [
            f"https://sp.netkeiba.com/race/shutuba.html?race_id={race_id}",
            f"https://m.netkeiba.com/race/shutuba.html?race_id={race_id}",
        ]
        
        for url in mobile_urls:
            try:
                response = self.session.get(url, headers=mobile_headers, timeout=10)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    parsed = self._parse_mobile_page(soup, race_id)
                    if parsed is not None:
                        return parsed
            except:
                continue
        
        return None
    
    def _get_data_from_api(self, race_id: str) -> Optional[pd.DataFrame]:
        """Ajax/API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        print("ğŸ”Œ API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ã®å–å¾—ã‚’è©¦è¡Œ...")
        
        api_endpoints = [
            f"https://race.netkeiba.com/api/api_get_odds.html?race_id={race_id}",
            f"https://race.netkeiba.com/api/race_before.html?race_id={race_id}",
            f"https://race.netkeiba.com/race/odds_ajax.html?race_id={race_id}",
        ]
        
        for endpoint in api_endpoints:
            try:
                # Ajax ãƒªã‚¯ã‚¨ã‚¹ãƒˆç”¨ãƒ˜ãƒƒãƒ€ãƒ¼
                ajax_headers = self.session.headers.copy()
                ajax_headers.update({
                    'X-Requested-With': 'XMLHttpRequest',
                    'Referer': f'https://race.netkeiba.com/race/shutuba.html?race_id={race_id}'
                })
                
                response = self.session.get(endpoint, headers=ajax_headers, timeout=10)
                
                if response.status_code == 200:
                    print(f"âœ“ API ãƒ¬ã‚¹ãƒãƒ³ã‚¹å–å¾—: {endpoint}")
                    
                    # JSON ã®å ´åˆ
                    try:
                        data = response.json()
                        parsed = self._parse_api_json(data, race_id)
                        if parsed is not None:
                            return parsed
                    except:
                        pass
                    
                    # HTML/text ã®å ´åˆ
                    if response.text:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        parsed = self._parse_api_html(soup, race_id)
                        if parsed is not None:
                            return parsed
                            
            except Exception as e:
                print(f"âš ï¸ API ã‚¨ãƒ©ãƒ¼ {endpoint}: {e}")
                continue
        
        return None
    
    def _parse_odds_json(self, data: Dict, race_id: str) -> Optional[pd.DataFrame]:
        """ã‚ªãƒƒã‚ºJSONãƒ‡ãƒ¼ã‚¿ã®è§£æ"""
        try:
            if 'odds' in data or 'horse' in data:
                # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«å¿œã˜ã¦è§£æ
                horses_data = []
                
                # ä¸€èˆ¬çš„ãªJSONãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ ã‚’æƒ³å®š
                if isinstance(data, dict) and 'data' in data:
                    data = data['data']
                
                if isinstance(data, list):
                    for item in data:
                        if isinstance(item, dict):
                            horse_data = self._extract_from_json_item(item, race_id)
                            if horse_data:
                                horses_data.append(horse_data)
                
                if horses_data:
                    print(f"âœ“ JSON ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºæˆåŠŸ: {len(horses_data)}é ­")
                    return pd.DataFrame(horses_data)
        except:
            pass
        
        return None
    
    def _parse_odds_html(self, soup: BeautifulSoup, race_id: str) -> Optional[pd.DataFrame]:
        """ã‚ªãƒƒã‚ºãƒšãƒ¼ã‚¸HTMLã®è§£æ"""
        try:
            # ã‚ªãƒƒã‚ºãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã™
            odds_tables = soup.find_all('table', class_=['odds_table', 'OddsTable', 'race_table'])
            
            for table in odds_tables:
                horses_data = []
                rows = table.find_all('tr')
                
                for row in rows[1:]:  # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 3:
                        horse_data = self._extract_from_odds_row(cells, race_id)
                        if horse_data:
                            horses_data.append(horse_data)
                
                if horses_data:
                    print(f"âœ“ ã‚ªãƒƒã‚ºHTMLã‹ã‚‰ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºæˆåŠŸ: {len(horses_data)}é ­")
                    return pd.DataFrame(horses_data)
        except:
            pass
        
        return None
    
    def _parse_shutuba_page(self, soup: BeautifulSoup, race_id: str) -> Optional[pd.DataFrame]:
        """å‡ºé¦¬è¡¨ãƒšãƒ¼ã‚¸ã®è§£æï¼ˆåŸºæœ¬æƒ…å ±ï¼‰"""
        try:
            table = soup.find('table', class_='Shutuba_Table')
            if not table:
                return None
            
            horses_data = []
            rows = table.find_all('tr')
            
            for row in rows:
                cells = row.find_all(['td', 'th'])
                if len(cells) >= 8:
                    # é¦¬ç•ªãƒã‚§ãƒƒã‚¯
                    first_cell = cells[0].get_text(strip=True)
                    if first_cell.isdigit() and 1 <= int(first_cell) <= 8:
                        # é¦¬ç•ªã¯å®Ÿéš›ã«ã¯ã‚»ãƒ«1ã«ã‚ã‚‹
                        if len(cells) > 1:
                            umaban_text = cells[1].get_text(strip=True)
                            if umaban_text.isdigit() and 1 <= int(umaban_text) <= 18:
                                horse_data = self._extract_basic_horse_data(cells, race_id)
                                if horse_data:
                                    horses_data.append(horse_data)
            
            if horses_data:
                print(f"âœ“ å‡ºé¦¬è¡¨ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºæˆåŠŸ: {len(horses_data)}é ­")
                return pd.DataFrame(horses_data)
        except:
            pass
        
        return None
    
    def _parse_mobile_page(self, soup: BeautifulSoup, race_id: str) -> Optional[pd.DataFrame]:
        """ãƒ¢ãƒã‚¤ãƒ«ãƒšãƒ¼ã‚¸ã®è§£æ"""
        # ãƒ¢ãƒã‚¤ãƒ«ç‰ˆã®ç°¡å˜ãªæ§‹é€ ã‚’è§£æ
        try:
            # ãƒ¢ãƒã‚¤ãƒ«ç‰ˆç‰¹æœ‰ã®ã‚¯ãƒ©ã‚¹ã‚’æ¢ã™
            mobile_selectors = [
                'div.horse_list',
                'table.horse_table',
                'div.race_horse',
                'ul.horse_list'
            ]
            
            for selector in mobile_selectors:
                elements = soup.select(selector)
                if elements:
                    # ãƒ¢ãƒã‚¤ãƒ«ç‰ˆã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯
                    pass
        except:
            pass
        
        return None
    
    def _parse_api_json(self, data: Dict, race_id: str) -> Optional[pd.DataFrame]:
        """API JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®è§£æ"""
        return self._parse_odds_json(data, race_id)
    
    def _parse_api_html(self, soup: BeautifulSoup, race_id: str) -> Optional[pd.DataFrame]:
        """API HTMLãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®è§£æ"""
        return self._parse_odds_html(soup, race_id)
    
    def _extract_from_json_item(self, item: Dict, race_id: str) -> Optional[Dict]:
        """JSON ã‚¢ã‚¤ãƒ†ãƒ ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿æŠ½å‡º"""
        try:
            return {
                'race_id': race_id,
                'æ ': item.get('waku', 1),
                'é¦¬ç•ª': item.get('umaban', item.get('horse_number', 1)),
                'é¦¬å': item.get('horse_name', item.get('name', 'ä¸æ˜')),
                'æ€§é½¢': item.get('age_sex', 'ä¸æ˜'),
                'é¨æ‰‹': item.get('jockey', item.get('jockey_name', 'ä¸æ˜')),
                'å©èˆ': item.get('trainer', 'ä¸æ˜'),
                'æ–¤é‡': float(item.get('weight', 57.0)),
                'é¦¬ä½“é‡': item.get('horse_weight', 'ä¸æ˜'),
                'ã‚ªãƒƒã‚º': float(item.get('odds', item.get('win_odds'))) if item.get('odds') else None,
                'äººæ°—': int(item.get('popularity', item.get('rank'))) if item.get('popularity') else None
            }
        except:
            return None
    
    def _extract_from_odds_row(self, cells: List, race_id: str) -> Optional[Dict]:
        """ã‚ªãƒƒã‚ºãƒ†ãƒ¼ãƒ–ãƒ«è¡Œã‹ã‚‰ãƒ‡ãƒ¼ã‚¿æŠ½å‡º"""
        try:
            umaban = int(cells[0].get_text(strip=True))
            horse_name = cells[1].get_text(strip=True) if len(cells) > 1 else "ä¸æ˜"
            odds_text = cells[2].get_text(strip=True) if len(cells) > 2 else None
            
            odds = None
            if odds_text and re.match(r'^\d+\.\d+$', odds_text):
                odds = float(odds_text)
            
            return {
                'race_id': race_id,
                'æ ': ((umaban - 1) // 2) + 1,
                'é¦¬ç•ª': umaban,
                'é¦¬å': horse_name,
                'æ€§é½¢': 'ä¸æ˜',
                'é¨æ‰‹': 'ä¸æ˜',
                'å©èˆ': 'ä¸æ˜',
                'æ–¤é‡': 57.0,
                'é¦¬ä½“é‡': 'ä¸æ˜',
                'ã‚ªãƒƒã‚º': odds,
                'äººæ°—': None
            }
        except:
            return None
    
    def _extract_basic_horse_data(self, cells: List, race_id: str) -> Optional[Dict]:
        """åŸºæœ¬çš„ãªé¦¬ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º"""
        try:
            # å‰å›ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’å†åˆ©ç”¨
            waku = 1
            waku_cell = cells[0]
            waku_text = waku_cell.get_text(strip=True)
            if waku_text.isdigit() and 1 <= int(waku_text) <= 8:
                waku = int(waku_text)
            
            umaban = int(cells[1].get_text(strip=True))
            horse_name = cells[3].get_text(strip=True) if len(cells) > 3 else "ä¸æ˜"
            sei_rei = cells[4].get_text(strip=True) if len(cells) > 4 else "ä¸æ˜"
            kinryo = float(cells[5].get_text(strip=True)) if len(cells) > 5 and re.match(r'^5[0-9]\.[05]$', cells[5].get_text(strip=True)) else 57.0
            jockey = cells[6].get_text(strip=True) if len(cells) > 6 else "ä¸æ˜"
            trainer = cells[7].get_text(strip=True) if len(cells) > 7 else "ä¸æ˜"
            
            # å©èˆåã‹ã‚‰åœ°åŸŸãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’é™¤å»
            trainer = re.sub(r'^(æ —æ±|ç¾æµ¦|ç¬ æ¾|é‡‘æ²¢|åœ’ç”°|å§«è·¯|é«˜çŸ¥|ä½è³€|é–€åˆ¥|ç››å²¡|æ°´æ²¢|æµ¦å’Œ|èˆ¹æ©‹|å¤§äº•|å·å´)', '', trainer)
            
            horse_weight = "ä¸æ˜"
            if len(cells) > 8:
                weight_text = cells[8].get_text(strip=True)
                if re.match(r'\d{3,4}\([+-]?\d+\)', weight_text):
                    horse_weight = weight_text
            
            # ã‚ªãƒƒã‚ºã¨äººæ°—ã‚’ç©æ¥µçš„ã«æ¢ã™
            odds = self._aggressive_odds_search(cells)
            popularity = self._aggressive_popularity_search(cells)
            
            return {
                'race_id': race_id,
                'æ ': waku,
                'é¦¬ç•ª': umaban,
                'é¦¬å': horse_name,
                'æ€§é½¢': sei_rei,
                'é¨æ‰‹': jockey,
                'å©èˆ': trainer,
                'æ–¤é‡': kinryo,
                'é¦¬ä½“é‡': horse_weight,
                'ã‚ªãƒƒã‚º': odds,
                'äººæ°—': popularity
            }
        except:
            return None
    
    def _aggressive_odds_search(self, cells: List) -> Optional[float]:
        """ç©æ¥µçš„ãªã‚ªãƒƒã‚ºæ¤œç´¢"""
        # å…¨ã‚»ãƒ«ã‚’ç·å½“ãŸã‚Šã§ãƒã‚§ãƒƒã‚¯
        for cell in cells:
            text = cell.get_text(strip=True)
            
            # æ•°å€¤ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åºƒãæ¢ã™
            patterns = [
                r'^(\d{1,3}\.\d{1,2})$',
                r'(\d+\.\d+)å€',
                r'å˜å‹.*?(\d+\.\d+)',
                r'win.*?(\d+\.\d+)',
                r'ã‚ªãƒƒã‚º.*?(\d+\.\d+)'
            ]
            
            for pattern in patterns:
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    try:
                        odds_val = float(match.group(1))
                        if 1.0 <= odds_val <= 999.0 and not (50.0 <= odds_val <= 60.0):
                            return odds_val
                    except:
                        continue
        
        return None
    
    def _aggressive_popularity_search(self, cells: List) -> Optional[int]:
        """ç©æ¥µçš„ãªäººæ°—æ¤œç´¢"""
        # å…¨ã‚»ãƒ«ã‚’ç·å½“ãŸã‚Šã§ãƒã‚§ãƒƒã‚¯
        for cell in cells:
            text = cell.get_text(strip=True)
            
            # äººæ°—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™
            patterns = [
                r'^(\d{1,2})$',
                r'(\d{1,2})äººæ°—',
                r'äººæ°—(\d{1,2})',
                r'rank.*?(\d{1,2})',
                r'pop.*?(\d{1,2})'
            ]
            
            for pattern in patterns:
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    try:
                        pop_val = int(match.group(1))
                        if 1 <= pop_val <= 18:
                            return pop_val
                    except:
                        continue
        
        return None
    
    def _merge_all_data(self, odds_data: Optional[pd.DataFrame], 
                       shutuba_data: Optional[pd.DataFrame], 
                       mobile_data: Optional[pd.DataFrame], 
                       api_data: Optional[pd.DataFrame], 
                       race_id: str) -> pd.DataFrame:
        """å…¨ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’çµ±åˆ"""
        
        # æœ€ã‚‚å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’é¸æŠ
        data_sources = [
            ("API", api_data),
            ("ã‚ªãƒƒã‚ºãƒšãƒ¼ã‚¸", odds_data), 
            ("å‡ºé¦¬è¡¨", shutuba_data),
            ("ãƒ¢ãƒã‚¤ãƒ«", mobile_data)
        ]
        
        best_data = None
        best_score = -1
        
        for name, data in data_sources:
            if data is not None and not data.empty:
                # ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆã‚ªãƒƒã‚ºã¨äººæ°—ã®å–å¾—æ•°ã§è©•ä¾¡ï¼‰
                odds_count = data['ã‚ªãƒƒã‚º'].notna().sum() if 'ã‚ªãƒƒã‚º' in data.columns else 0
                pop_count = data['äººæ°—'].notna().sum() if 'äººæ°—' in data.columns else 0
                horse_count = len(data)
                
                score = odds_count * 3 + pop_count * 2 + horse_count
                print(f"ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è©•ä¾¡ {name}: é¦¬{horse_count}é ­, ã‚ªãƒƒã‚º{odds_count}é ­, äººæ°—{pop_count}é ­, ã‚¹ã‚³ã‚¢{score}")
                
                if score > best_score:
                    best_score = score
                    best_data = data
        
        return best_data if best_data is not None else pd.DataFrame()


def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ã‚ªãƒƒã‚ºç‰¹åŒ–netkeiba.comã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼')
    parser.add_argument('race_id', type=str, help='ãƒ¬ãƒ¼ã‚¹ID (ä¾‹: 202406020311)')
    parser.add_argument('--output', type=str, default='odds_focused_data.csv', help='å‡ºåŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«')
    
    args = parser.parse_args()
    
    if not args.race_id.isdigit() or len(args.race_id) != 12:
        print("âŒ ãƒ¬ãƒ¼ã‚¹IDã¯12æ¡ã®æ•°å­—ã§å…¥åŠ›ã—ã¦ãã ã•ã„")
        return
    
    scraper = OddsFocusedScraper()
    race_data = scraper.scrape_race_with_odds(args.race_id)
    
    if race_data.empty:
        print("âŒ ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    # CSVä¿å­˜
    race_data.to_csv(args.output, index=False, encoding='utf-8')
    print(f"\nğŸ’¾ ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {args.output}")
    
    # çµæœè¡¨ç¤º
    print(f"\nğŸ“Š å–å¾—çµæœ: {len(race_data)}é ­")
    print("\nğŸ‡ å‡ºé¦¬è¡¨:")
    for _, horse in race_data.iterrows():
        odds_str = f"{horse['ã‚ªãƒƒã‚º']}å€" if horse['ã‚ªãƒƒã‚º'] is not None else "æœªè¨­å®š"
        pop_str = f"{horse['äººæ°—']}äººæ°—" if horse['äººæ°—'] is not None else "æœªè¨­å®š"
        print(f"  {horse['æ ']}æ {horse['é¦¬ç•ª']:2d}ç•ª {horse['é¦¬å']:15s} "
              f"{horse['é¨æ‰‹']:8s} {horse['å©èˆ']:8s} {horse['é¦¬ä½“é‡']:10s} "
              f"{odds_str:8s} {pop_str}")


if __name__ == "__main__":
    main()