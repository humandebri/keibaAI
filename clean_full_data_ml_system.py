#!/usr/bin/env python3
"""
ã‚¯ãƒªãƒ¼ãƒ³ãªå…¨19ä¸‡ä»¶ãƒ‡ãƒ¼ã‚¿æ´»ç”¨ã‚·ã‚¹ãƒ†ãƒ 
ã‚ªãƒƒã‚ºã‚’ä½¿ã‚ãªã„çœŸã®æ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ 
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score, accuracy_score
import joblib
import warnings
warnings.filterwarnings('ignore')

class CleanFullDataMLSystem:
    """ã‚ªãƒƒã‚ºã‚’ä½¿ã‚ãªã„å…¨ãƒ‡ãƒ¼ã‚¿æ´»ç”¨æ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.model = None
        self.scaler = StandardScaler()
        self.feature_columns = None
        self.feature_importance = None
        self.model_metrics = {}
        self.is_trained = False
        # è¨“ç·´æ™‚ã«å­¦ç¿’ã—ãŸå®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        self.jockey_stats = {}
        self.trainer_stats = {}
        
    def load_full_training_data(self):
        """å…¨19ä¸‡ä»¶ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        print("ğŸ“Š å…¨19ä¸‡ä»¶ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹")
        
        try:
            # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            df = pd.read_csv("encoded/2020_2025encoded_data_v2.csv")
            print(f"   èª­ã¿è¾¼ã¿å®Œäº†: {len(df):,}ä»¶")
            print(f"   å‹åˆ©ã‚±ãƒ¼ã‚¹: {(df['ç€é †'] == 1).sum():,}ä»¶ ({(df['ç€é †'] == 1).mean():.3f})")
            print(f"   ç‰¹å¾´é‡æ•°: {len(df.columns)}å€‹")
            print(f"   æœŸé–“: 2020-2025å¹´ã®å…¨ãƒ‡ãƒ¼ã‚¿")
            
            return df
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def create_clean_features(self, df: pd.DataFrame) -> tuple:
        """ã‚ªãƒƒã‚ºã‚’ä½¿ã‚ãªã„ã‚¯ãƒªãƒ¼ãƒ³ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"""
        print("ğŸ”§ ã‚¯ãƒªãƒ¼ãƒ³ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°é–‹å§‹")
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°
        y = (df['ç€é †'] == 1).astype(int)
        
        # ç‰¹å¾´é‡é¸æŠï¼ˆã‚ªãƒƒã‚ºé–¢é€£ã‚’é™¤å¤–ï¼‰
        selected_features = []
        
        # 1. åŸºæœ¬æƒ…å ±ï¼ˆã‚ªãƒƒã‚ºé™¤å¤–ï¼‰
        basic_features = [
            'ä½“é‡', 'ä½“é‡å¤‰åŒ–', 'æ–¤é‡', 'ä¸ŠãŒã‚Š', 
            'å‡ºèµ°é ­æ•°', 'è·é›¢', 'ã‚¯ãƒ©ã‚¹', 'é¨æ‰‹ã®å‹ç‡', 'æ€§', 'é½¢'
        ]
        
        # 2. éå»æˆç¸¾ï¼ˆã‚ªãƒƒã‚ºé™¤å¤–ï¼‰
        past_features = []
        for i in range(1, 6):  # éå»5èµ°
            past_features.extend([f'ç€é †{i}', f'è·é›¢{i}', f'é€šéé †{i}', f'èµ°ç ´æ™‚é–“{i}'])
        
        # 3. æ™‚ç³»åˆ—æƒ…å ±ï¼ˆä¼‘é¤ŠæœŸé–“ï¼‰
        temporal_features = ['æ—¥ä»˜å·®1', 'æ—¥ä»˜å·®2', 'æ—¥ä»˜å·®3']
        
        # 4. çµ±è¨ˆç‰¹å¾´é‡
        stat_features = ['å¹³å‡ã‚¹ãƒ”ãƒ¼ãƒ‰', 'éå»5èµ°ã®åˆè¨ˆè³é‡‘', 'å¹³å‡æ–¤é‡']
        
        # 5. ãƒ¬ãƒ¼ã‚¹æ¡ä»¶
        race_condition_features = ['èŠãƒ»ãƒ€ãƒ¼ãƒˆ', 'å›ã‚Š', 'é¦¬å ´', 'å¤©æ°—', 'å ´id']
        
        # å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ç‰¹å¾´é‡ã®ã¿é¸æŠ
        all_candidates = basic_features + past_features + temporal_features + stat_features + race_condition_features
        available_features = [col for col in all_candidates if col in df.columns]
        selected_features.extend(available_features)
        
        print(f"   åŸºæœ¬ç‰¹å¾´é‡: {len(available_features)}å€‹")
        
        # 6. æ–°è¦è¨ˆç®—ç‰¹å¾´é‡ï¼ˆè¦æ±‚ã•ã‚ŒãŸç‰¹å¾´é‡ã‚’è¿½åŠ ï¼‰
        enhanced_df = df.copy()
        
        # é¦¬ã®éå»æˆç¸¾åˆ†æï¼ˆã‚ªãƒƒã‚ºé™¤å¤–ï¼‰
        if all(f'ç€é †{i}' in df.columns for i in range(1, 6)):
            past_positions = []
            past_times = []
            
            for i in range(1, 6):
                if f'ç€é †{i}' in df.columns:
                    past_positions.append(df[f'ç€é †{i}'].fillna(10))
                if f'èµ°ç ´æ™‚é–“{i}' in df.columns:
                    past_times.append(df[f'èµ°ç ´æ™‚é–“{i}'].fillna(120))
            
            if past_positions:
                past_pos_df = pd.concat(past_positions, axis=1)
                enhanced_df['éå»å¹³å‡ç€é †'] = past_pos_df.mean(axis=1)
                enhanced_df['éå»æœ€é«˜ç€é †'] = past_pos_df.min(axis=1)
                enhanced_df['å‹åˆ©çµŒé¨“'] = (past_pos_df == 1).sum(axis=1)
                enhanced_df['è¤‡å‹çµŒé¨“'] = (past_pos_df <= 3).sum(axis=1)
                enhanced_df['ç€é †å®‰å®šæ€§'] = past_pos_df.std(axis=1).fillna(5)
                enhanced_df['ç€é †æ”¹å–„å‚¾å‘'] = past_pos_df.iloc[:, 0] - past_pos_df.iloc[:, -1]  # å‰èµ°-æœ€å¤èµ°
                selected_features.extend(['éå»å¹³å‡ç€é †', 'éå»æœ€é«˜ç€é †', 'å‹åˆ©çµŒé¨“', 'è¤‡å‹çµŒé¨“', 'ç€é †å®‰å®šæ€§', 'ç€é †æ”¹å–„å‚¾å‘'])
            
            if past_times:
                past_times_df = pd.concat(past_times, axis=1)
                enhanced_df['éå»å¹³å‡ã‚¿ã‚¤ãƒ '] = past_times_df.mean(axis=1)
                enhanced_df['éå»æœ€é«˜ã‚¿ã‚¤ãƒ '] = past_times_df.min(axis=1)
                enhanced_df['ã‚¿ã‚¤ãƒ å®‰å®šæ€§'] = past_times_df.std(axis=1).fillna(10)
                selected_features.extend(['éå»å¹³å‡ã‚¿ã‚¤ãƒ ', 'éå»æœ€é«˜ã‚¿ã‚¤ãƒ ', 'ã‚¿ã‚¤ãƒ å®‰å®šæ€§'])
        
        # å‰èµ°ã‹ã‚‰ã®ä¼‘é¤ŠæœŸé–“ï¼ˆè¦æ±‚ç‰¹å¾´é‡ï¼‰
        if 'æ—¥ä»˜å·®1' in df.columns:
            enhanced_df['ä¼‘é¤ŠæœŸé–“'] = df['æ—¥ä»˜å·®1'].fillna(30)
            enhanced_df['ä¼‘é¤Šé©æ­£'] = np.where(
                (enhanced_df['ä¼‘é¤ŠæœŸé–“'] >= 14) & (enhanced_df['ä¼‘é¤ŠæœŸé–“'] <= 60), 1.2,
                np.where(enhanced_df['ä¼‘é¤ŠæœŸé–“'] < 14, 0.9, 0.8)
            )
            # ä¼‘é¤ŠæœŸé–“ã®ã‚«ãƒ†ã‚´ãƒªåŒ–
            enhanced_df['ä¼‘é¤Šã‚«ãƒ†ã‚´ãƒª'] = pd.cut(enhanced_df['ä¼‘é¤ŠæœŸé–“'], 
                                          bins=[0, 7, 14, 30, 60, 180, 1000],
                                          labels=[1, 2, 3, 4, 5, 6]).astype(float)
            selected_features.extend(['ä¼‘é¤ŠæœŸé–“', 'ä¼‘é¤Šé©æ­£', 'ä¼‘é¤Šã‚«ãƒ†ã‚´ãƒª'])
        
        # è·é›¢é©æ€§åˆ†æï¼ˆè¡€çµ±æƒ…å ±ã®ä»£æ›¿ï¼‰
        if 'è·é›¢' in df.columns:
            current_distance = df['è·é›¢'].fillna(1600)
            enhanced_df['è·é›¢ã‚«ãƒ†ã‚´ãƒª'] = pd.cut(current_distance, 
                                        bins=[0, 1400, 1800, 2200, 3000], 
                                        labels=[1, 2, 3, 4]).astype(float)
            
            # åŒè·é›¢çµŒé¨“ï¼ˆè¡€çµ±ã®ä»£æ›¿æŒ‡æ¨™ï¼‰
            if all(f'è·é›¢{i}' in df.columns for i in range(1, 4)):
                same_dist_exp = 0
                for i in range(1, 4):
                    same_dist_exp += (df[f'è·é›¢{i}'] == current_distance).astype(int).fillna(0)
                enhanced_df['åŒè·é›¢çµŒé¨“'] = same_dist_exp / 3
                
                # è·é›¢å¤‰åŒ–ã®åˆ†æ
                if 'è·é›¢1' in df.columns:
                    enhanced_df['è·é›¢å¤‰åŒ–'] = current_distance - df['è·é›¢1'].fillna(current_distance)
                    enhanced_df['è·é›¢å»¶é•·'] = (enhanced_df['è·é›¢å¤‰åŒ–'] > 200).astype(int)
                    enhanced_df['è·é›¢çŸ­ç¸®'] = (enhanced_df['è·é›¢å¤‰åŒ–'] < -200).astype(int)
                    selected_features.extend(['è·é›¢å¤‰åŒ–', 'è·é›¢å»¶é•·', 'è·é›¢çŸ­ç¸®'])
                
                selected_features.extend(['è·é›¢ã‚«ãƒ†ã‚´ãƒª', 'åŒè·é›¢çµŒé¨“'])
        
        # é¨æ‰‹ãƒ»èª¿æ•™å¸«è©³ç´°æˆç¸¾ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å®Ÿéš›ã«è¨ˆç®—ï¼‰
        if 'é¨æ‰‹' in df.columns:
            print("   ğŸ‡ å…¨ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é¨æ‰‹å®Ÿç¸¾è¨ˆç®—ä¸­...")
            jockey_stats = df.groupby('é¨æ‰‹').agg({
                'ç€é †': ['count', lambda x: (x == 1).sum(), lambda x: (x <= 3).sum()],
                'ä¸ŠãŒã‚Š': 'mean'
            }).round(4)
            jockey_stats.columns = ['é¨ä¹—æ•°', 'å‹åˆ©æ•°', 'è¤‡å‹æ•°', 'å¹³å‡ä¸ŠãŒã‚Š']
            jockey_stats['å®Ÿå‹ç‡'] = jockey_stats['å‹åˆ©æ•°'] / jockey_stats['é¨ä¹—æ•°']
            jockey_stats['å®Ÿè¤‡å‹ç‡'] = jockey_stats['è¤‡å‹æ•°'] / jockey_stats['é¨ä¹—æ•°']
            
            # å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒ©ã‚¹å¤‰æ•°ã«ä¿å­˜ï¼ˆäºˆæ¸¬æ™‚ã«ä½¿ç”¨ï¼‰
            self.jockey_stats = {
                'å®Ÿå‹ç‡': dict(zip(jockey_stats.index, jockey_stats['å®Ÿå‹ç‡'])),
                'å®Ÿè¤‡å‹ç‡': dict(zip(jockey_stats.index, jockey_stats['å®Ÿè¤‡å‹ç‡'])),
                'é¨ä¹—æ•°': dict(zip(jockey_stats.index, jockey_stats['é¨ä¹—æ•°'])),
                'å¹³å‡ä¸ŠãŒã‚Š': dict(zip(jockey_stats.index, jockey_stats['å¹³å‡ä¸ŠãŒã‚Š']))
            }
            
            enhanced_df['é¨æ‰‹å®Ÿå‹ç‡'] = df['é¨æ‰‹'].map(jockey_stats['å®Ÿå‹ç‡']).fillna(0.08)
            enhanced_df['é¨æ‰‹å®Ÿè¤‡å‹ç‡'] = df['é¨æ‰‹'].map(jockey_stats['å®Ÿè¤‡å‹ç‡']).fillna(0.25)
            enhanced_df['é¨æ‰‹é¨ä¹—æ•°'] = df['é¨æ‰‹'].map(jockey_stats['é¨ä¹—æ•°']).fillna(50)
            enhanced_df['é¨æ‰‹å¹³å‡ä¸ŠãŒã‚Š'] = df['é¨æ‰‹'].map(jockey_stats['å¹³å‡ä¸ŠãŒã‚Š']).fillna(35.0)
            selected_features.extend(['é¨æ‰‹å®Ÿå‹ç‡', 'é¨æ‰‹å®Ÿè¤‡å‹ç‡', 'é¨æ‰‹é¨ä¹—æ•°', 'é¨æ‰‹å¹³å‡ä¸ŠãŒã‚Š'])
        
        if 'èª¿æ•™å¸«' in df.columns:
            print("   ğŸ‘” å…¨ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰èª¿æ•™å¸«å®Ÿç¸¾è¨ˆç®—ä¸­...")
            trainer_stats = df.groupby('èª¿æ•™å¸«').agg({
                'ç€é †': ['count', lambda x: (x == 1).sum(), lambda x: (x <= 3).sum()],
                'ä½“é‡': 'mean'
            }).round(4)
            trainer_stats.columns = ['ç®¡ç†æ•°', 'å‹åˆ©æ•°', 'è¤‡å‹æ•°', 'å¹³å‡ä½“é‡']
            trainer_stats['å®Ÿå‹ç‡'] = trainer_stats['å‹åˆ©æ•°'] / trainer_stats['ç®¡ç†æ•°']
            trainer_stats['å®Ÿè¤‡å‹ç‡'] = trainer_stats['è¤‡å‹æ•°'] / trainer_stats['ç®¡ç†æ•°']
            
            # å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒ©ã‚¹å¤‰æ•°ã«ä¿å­˜ï¼ˆäºˆæ¸¬æ™‚ã«ä½¿ç”¨ï¼‰
            self.trainer_stats = {
                'å®Ÿå‹ç‡': dict(zip(trainer_stats.index, trainer_stats['å®Ÿå‹ç‡'])),
                'å®Ÿè¤‡å‹ç‡': dict(zip(trainer_stats.index, trainer_stats['å®Ÿè¤‡å‹ç‡'])),
                'ç®¡ç†æ•°': dict(zip(trainer_stats.index, trainer_stats['ç®¡ç†æ•°'])),
                'å¹³å‡ä½“é‡': dict(zip(trainer_stats.index, trainer_stats['å¹³å‡ä½“é‡']))
            }
            
            enhanced_df['èª¿æ•™å¸«å®Ÿå‹ç‡'] = df['èª¿æ•™å¸«'].map(trainer_stats['å®Ÿå‹ç‡']).fillna(0.06)
            enhanced_df['èª¿æ•™å¸«å®Ÿè¤‡å‹ç‡'] = df['èª¿æ•™å¸«'].map(trainer_stats['å®Ÿè¤‡å‹ç‡']).fillna(0.20)
            enhanced_df['èª¿æ•™å¸«ç®¡ç†æ•°'] = df['èª¿æ•™å¸«'].map(trainer_stats['ç®¡ç†æ•°']).fillna(100)
            enhanced_df['èª¿æ•™å¸«å¹³å‡ä½“é‡'] = df['èª¿æ•™å¸«'].map(trainer_stats['å¹³å‡ä½“é‡']).fillna(480)
            selected_features.extend(['èª¿æ•™å¸«å®Ÿå‹ç‡', 'èª¿æ•™å¸«å®Ÿè¤‡å‹ç‡', 'èª¿æ•™å¸«ç®¡ç†æ•°', 'èª¿æ•™å¸«å¹³å‡ä½“é‡'])
        
        # ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †ä½åˆ†æï¼ˆè¦æ±‚ç‰¹å¾´é‡ï¼‰
        if any(f'é€šéé †{i}' in df.columns for i in range(1, 4)):
            corner_positions = []
            for i in range(1, 4):
                if f'é€šéé †{i}' in df.columns:
                    corner_positions.append(df[f'é€šéé †{i}'].fillna(8))
            
            if corner_positions:
                corner_df = pd.concat(corner_positions, axis=1)
                enhanced_df['å¹³å‡é€šéé †'] = corner_df.mean(axis=1)
                enhanced_df['4è§’ä½ç½®'] = corner_positions[0] if corner_positions else 8
                enhanced_df['ä½ç½®å–ã‚Šèƒ½åŠ›'] = 10 - enhanced_df['å¹³å‡é€šéé †']  # é«˜ã„ã»ã©è‰¯ã„
                enhanced_df['é€šéé †å®‰å®šæ€§'] = corner_df.std(axis=1).fillna(3)
                enhanced_df['å‰åŠå¾ŒåŠå·®'] = corner_df.iloc[:, -1] - corner_df.iloc[:, 0] if len(corner_positions) >= 2 else 0
                selected_features.extend(['å¹³å‡é€šéé †', '4è§’ä½ç½®', 'ä½ç½®å–ã‚Šèƒ½åŠ›', 'é€šéé †å®‰å®šæ€§', 'å‰åŠå¾ŒåŠå·®'])
        
        # æ ç•ªè©³ç´°åˆ†æï¼ˆè¦æ±‚ç‰¹å¾´é‡ï¼‰
        if 'æ ' in df.columns:
            enhanced_df['æ ç•ª'] = df['æ '].fillna(4)
            enhanced_df['å†…æ '] = (enhanced_df['æ ç•ª'] <= 3).astype(int)
            enhanced_df['å¤–æ '] = (enhanced_df['æ ç•ª'] >= 7).astype(int)
            enhanced_df['ä¸­æ '] = ((enhanced_df['æ ç•ª'] >= 4) & (enhanced_df['æ ç•ª'] <= 6)).astype(int)
            
            # è·é›¢åˆ¥æ æœ‰åˆ©åº¦
            if 'è·é›¢' in df.columns:
                distance = df['è·é›¢'].fillna(1600)
                enhanced_df['æ è·é›¢é©æ€§'] = np.where(
                    distance <= 1400,
                    np.where(enhanced_df['æ ç•ª'] <= 4, 1.1, 0.9),  # çŸ­è·é›¢ã¯å†…æ æœ‰åˆ©
                    np.where(distance >= 2000,
                             np.where(enhanced_df['æ ç•ª'] >= 5, 1.05, 0.95),  # é•·è·é›¢ã¯å¤–æ ã‚„ã‚„æœ‰åˆ©
                             1.0)  # ä¸­è·é›¢ã¯ä¸­ç«‹
                )
            selected_features.extend(['æ ç•ª', 'å†…æ ', 'å¤–æ ', 'ä¸­æ ', 'æ è·é›¢é©æ€§'])
        
        # ä½“é‡ãƒ»ä½“èª¿åˆ†æ
        if 'ä½“é‡' in df.columns and 'ä½“é‡å¤‰åŒ–' in df.columns:
            enhanced_df['ä½“é‡é©æ­£'] = ((enhanced_df['ä½“é‡'] >= 450) & (enhanced_df['ä½“é‡'] <= 520)).astype(int)
            enhanced_df['ä½“é‡å¤‰åŒ–çµ¶å¯¾å€¤'] = abs(enhanced_df['ä½“é‡å¤‰åŒ–'].fillna(0))
            enhanced_df['ä½“é‡å¢—åŠ '] = (enhanced_df['ä½“é‡å¤‰åŒ–'] > 5).astype(int)
            enhanced_df['ä½“é‡æ¸›å°‘'] = (enhanced_df['ä½“é‡å¤‰åŒ–'] < -5).astype(int)
            selected_features.extend(['ä½“é‡é©æ­£', 'ä½“é‡å¤‰åŒ–çµ¶å¯¾å€¤', 'ä½“é‡å¢—åŠ ', 'ä½“é‡æ¸›å°‘'])
        
        # å¹´é½¢ãƒ»æ€§åˆ¥åˆ†æ
        if 'é½¢' in df.columns:
            # æ•°å€¤å‹ã«å¤‰æ›
            age = pd.to_numeric(enhanced_df['é½¢'], errors='coerce').fillna(4)
            enhanced_df['å¹´é½¢ãƒ”ãƒ¼ã‚¯'] = ((age >= 4) & (age <= 5)).astype(int)
            enhanced_df['è‹¥é¦¬'] = (age == 3).astype(int)
            enhanced_df['å¤é¦¬'] = (age >= 6).astype(int)
            selected_features.extend(['å¹´é½¢ãƒ”ãƒ¼ã‚¯', 'è‹¥é¦¬', 'å¤é¦¬'])
        
        # ç·åˆæŒ‡æ¨™ï¼ˆã‚ªãƒƒã‚ºä½¿ã‚ãªã„ï¼‰
        ability_components = []
        if 'éå»å¹³å‡ç€é †' in enhanced_df.columns:
            ability_components.append((10 - enhanced_df['éå»å¹³å‡ç€é †']) / 10)
        if 'é¨æ‰‹å®Ÿå‹ç‡' in enhanced_df.columns:
            ability_components.append(enhanced_df['é¨æ‰‹å®Ÿå‹ç‡'] * 5)
        if 'ä½ç½®å–ã‚Šèƒ½åŠ›' in enhanced_df.columns:
            ability_components.append(enhanced_df['ä½ç½®å–ã‚Šèƒ½åŠ›'] / 10)
        if 'èª¿æ•™å¸«å®Ÿå‹ç‡' in enhanced_df.columns:
            ability_components.append(enhanced_df['èª¿æ•™å¸«å®Ÿå‹ç‡'] * 5)
        if 'éå»æœ€é«˜ã‚¿ã‚¤ãƒ ' in enhanced_df.columns:
            ability_components.append((130 - enhanced_df['éå»æœ€é«˜ã‚¿ã‚¤ãƒ ']) / 20)  # ã‚¿ã‚¤ãƒ ã‚’èƒ½åŠ›æŒ‡æ¨™ã«
        
        if len(ability_components) >= 3:
            enhanced_df['ç·åˆèƒ½åŠ›æŒ‡æ¨™'] = pd.concat(ability_components, axis=1).mean(axis=1)
            selected_features.append('ç·åˆèƒ½åŠ›æŒ‡æ¨™')
        
        # äººæ°—é–¢é€£ç‰¹å¾´é‡ã‚’é™¤å¤–ï¼ˆç´”ç²‹ãªæ©Ÿæ¢°å­¦ç¿’ã®ãŸã‚ï¼‰
        # ã‚ªãƒƒã‚ºã‚‚äººæ°—ã‚‚ä½¿ã‚ãªã„å®Œå…¨ã‚¯ãƒªãƒ¼ãƒ³ãªML
        
        # æœ€çµ‚ç‰¹å¾´é‡ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ä½œæˆ
        final_features = []
        for col in selected_features:
            if col in enhanced_df.columns:
                final_features.append(col)
        
        X = enhanced_df[final_features].copy()
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        X = X.replace([np.inf, -np.inf], 0)
        X = X.fillna(0)
        
        # ç‰¹å¾´é‡ã®æ¨™æº–åŒ–å‰ãƒã‚§ãƒƒã‚¯
        X = X.select_dtypes(include=[np.number])  # æ•°å€¤å‹ã®ã¿
        
        self.feature_columns = list(X.columns)
        
        print(f"âœ… ã‚¯ãƒªãƒ¼ãƒ³ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Œäº†")
        print(f"   æœ€çµ‚ç‰¹å¾´é‡æ•°: {len(self.feature_columns)}å€‹")
        print(f"   ä¸»è¦ç‰¹å¾´é‡: {self.feature_columns[:10]}")
        print(f"   ã‚ªãƒƒã‚ºé–¢é€£ç‰¹å¾´é‡: é™¤å¤–æ¸ˆã¿")
        
        return X, y
    
    def train_clean_model(self, X: pd.DataFrame, y: pd.Series):
        """ã‚¯ãƒªãƒ¼ãƒ³ãªãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
        print("ğŸ¤– ã‚¯ãƒªãƒ¼ãƒ³ãªãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹")
        
        # è¨“ç·´ãƒ»æ¤œè¨¼åˆ†å‰²ï¼ˆå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ç”¨ï¼‰
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.15, random_state=42, stratify=y
        )
        
        print(f"   è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {X_train.shape[0]:,}ä»¶")
        print(f"   æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {X_test.shape[0]:,}ä»¶") 
        print(f"   ç‰¹å¾´é‡æ•°: {X_train.shape[1]}å€‹")
        print(f"   æ­£ä¾‹ã®å‰²åˆ: {y_train.mean():.3f}")
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        print("   ğŸ“Š ç‰¹å¾´é‡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¸­...")
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # ã‚¯ãƒªãƒ¼ãƒ³ãªRandomForestï¼ˆã‚ªãƒƒã‚ºã«ä¾å­˜ã—ãªã„ï¼‰
        self.model = RandomForestClassifier(
            n_estimators=300,  # ã‚ªãƒƒã‚ºãªã—ãªã®ã§æœ¨ã‚’å¢—ã‚„ã™
            max_depth=20,      # ã‚ˆã‚Šæ·±ã„å­¦ç¿’ãŒå¿…è¦
            min_samples_split=80,
            min_samples_leaf=40,
            max_features='sqrt',
            random_state=42,
            class_weight='balanced',
            n_jobs=-1,
            oob_score=True
        )
        
        # è¨“ç·´
        print("   ğŸ”„ ã‚¯ãƒªãƒ¼ãƒ³ãªRandomForestè¨“ç·´ä¸­...")
        self.model.fit(X_train_scaled, y_train)
        
        # è©•ä¾¡
        test_pred = self.model.predict(X_test_scaled)
        test_proba = self.model.predict_proba(X_test_scaled)[:, 1]
        
        test_accuracy = accuracy_score(y_test, test_pred)
        test_auc = roc_auc_score(y_test, test_proba)
        oob_score = self.model.oob_score_
        
        self.model_metrics = {
            'model_name': 'CleanRandomForest',
            'test_accuracy': test_accuracy,
            'test_auc': test_auc,
            'oob_score': oob_score,
            'feature_count': len(self.feature_columns),
            'training_samples': len(X_train),
            'total_data_used': len(X)
        }
        
        self.is_trained = True
        
        # ç‰¹å¾´é‡é‡è¦åº¦
        self.feature_importance = pd.DataFrame({
            'feature': self.feature_columns,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print(f"âœ… ã‚¯ãƒªãƒ¼ãƒ³ãªãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†")
        print(f"   æ¤œè¨¼ç²¾åº¦: {test_accuracy:.3f}")
        print(f"   æ¤œè¨¼AUC: {test_auc:.3f}")
        print(f"   OOBç²¾åº¦: {oob_score:.3f}")
        
        print(f"\nğŸ“Š ã‚¯ãƒªãƒ¼ãƒ³ãªç‰¹å¾´é‡é‡è¦åº¦ Top 15:")
        for _, row in self.feature_importance.head(15).iterrows():
            print(f"      {row['feature']}: {row['importance']:.4f}")
        
        return self.model
    
    def predict_with_clean_ml(self, live_race_data: pd.DataFrame):
        """ã‚¯ãƒªãƒ¼ãƒ³ãªMLäºˆæ¸¬"""
        if not self.is_trained:
            print("âŒ ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return None
        
        print("ğŸ¯ ã‚¯ãƒªãƒ¼ãƒ³ãªMLäºˆæ¸¬å®Ÿè¡Œ")
        
        # ãƒ©ã‚¤ãƒ–ãƒ‡ãƒ¼ã‚¿ç‰¹å¾´é‡ä½œæˆ
        live_features = self._create_clean_live_features(live_race_data)
        
        if live_features is None:
            return None
        
        # äºˆæ¸¬
        X_live_scaled = self.scaler.transform(live_features)
        win_probabilities = self.model.predict_proba(X_live_scaled)[:, 1]
        
        # ç¢ºç‡æ­£è¦åŒ–
        win_probabilities = win_probabilities / win_probabilities.sum()
        
        # çµæœä½œæˆï¼ˆã‚ªãƒƒã‚ºã¯è¡¨ç¤ºã®ã¿ä½¿ç”¨ï¼‰
        results = live_race_data.copy()
        results['ã‚¯ãƒªãƒ¼ãƒ³MLå‹åˆ©ç¢ºç‡'] = win_probabilities
        results['ã‚¯ãƒªãƒ¼ãƒ³MLæœŸå¾…å€¤'] = win_probabilities * results['å˜å‹ã‚ªãƒƒã‚º'].astype(float)
        
        # ç€é †äºˆæ¸¬
        results['ã‚¯ãƒªãƒ¼ãƒ³MLæœŸå¾…ç€é †'] = (
            win_probabilities * 1 +
            (1 - win_probabilities) * (len(results) + 1) / 2
        )
        results['ã‚¯ãƒªãƒ¼ãƒ³MLäºˆæ¸¬ç€é †'] = results['ã‚¯ãƒªãƒ¼ãƒ³MLæœŸå¾…ç€é †'].rank().astype(int)
        
        # ã‚½ãƒ¼ãƒˆï¼ˆç¢ºç‡é †ï¼‰
        results = results.sort_values('ã‚¯ãƒªãƒ¼ãƒ³MLå‹åˆ©ç¢ºç‡', ascending=False)
        
        print("âœ… ã‚¯ãƒªãƒ¼ãƒ³ãªMLäºˆæ¸¬å®Œäº†")
        return results
    
    def _create_clean_live_features(self, live_race_data: pd.DataFrame):
        """ã‚¯ãƒªãƒ¼ãƒ³ãªãƒ©ã‚¤ãƒ–ç‰¹å¾´é‡ä½œæˆï¼ˆã‚ªãƒƒã‚ºä½¿ã‚ãªã„ï¼‰"""
        # åŸºæœ¬å¤‰æ›ï¼ˆCSVã®å®Ÿéš›ã®åˆ—åã‚’ä½¿ç”¨ï¼‰
        enhanced_df = live_race_data.copy()
        enhanced_df['é¦¬ä½“é‡_æ•°å€¤'] = enhanced_df['é¦¬ä½“é‡'].astype(float)
        
        if enhanced_df['é¦¬ä½“é‡å¤‰åŒ–'].dtype == 'object':
            enhanced_df['é¦¬ä½“é‡å¤‰åŒ–_æ•°å€¤'] = enhanced_df['é¦¬ä½“é‡å¤‰åŒ–'].astype(str).str.replace('+', '').astype(float)
        else:
            enhanced_df['é¦¬ä½“é‡å¤‰åŒ–_æ•°å€¤'] = enhanced_df['é¦¬ä½“é‡å¤‰åŒ–'].astype(float)
        
        # CSVã®åˆ—åã‚’æ¨™æº–åŒ–
        enhanced_df['è·é›¢_æ•°å€¤'] = enhanced_df['distance'].astype(float)
        enhanced_df['ã‚¯ãƒ©ã‚¹_æ•°å€¤'] = enhanced_df['class']
        enhanced_df['èŠãƒ€ãƒ¼ãƒˆ_æ•°å€¤'] = enhanced_df['surface']
        enhanced_df['æ–¤é‡_æ•°å€¤'] = enhanced_df['æ–¤é‡'].astype(float)
        
        # è¨“ç·´æ™‚ç‰¹å¾´é‡ã«åˆã‚ã›ãŸãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆã‚ªãƒƒã‚ºãƒ»äººæ°—é™¤å¤–ï¼‰
        live_features = pd.DataFrame()
        
        for feature in self.feature_columns:
            if feature == 'ä½“é‡':
                live_features[feature] = enhanced_df['é¦¬ä½“é‡_æ•°å€¤']
            elif feature == 'ä½“é‡å¤‰åŒ–':
                live_features[feature] = enhanced_df['é¦¬ä½“é‡å¤‰åŒ–_æ•°å€¤']
            elif feature == 'å‡ºèµ°é ­æ•°':
                live_features[feature] = len(enhanced_df)
            elif feature == 'è·é›¢':
                live_features[feature] = enhanced_df['è·é›¢_æ•°å€¤']
            elif feature == 'ã‚¯ãƒ©ã‚¹':
                # ã‚¯ãƒ©ã‚¹æƒ…å ±ã‚’æ•°å€¤åŒ–
                class_mapping = {
                    'æ–°é¦¬': 1, 'æœªå‹åˆ©': 2, '1å‹ã‚¯ãƒ©ã‚¹': 3, '2å‹ã‚¯ãƒ©ã‚¹': 4, '3å‹ã‚¯ãƒ©ã‚¹': 5,
                    'ã‚ªãƒ¼ãƒ—ãƒ³': 6, '4æ­³ä»¥ä¸Šã‚ªãƒ¼ãƒ—ãƒ³': 6, 'G3': 7, 'G2': 8, 'G1': 9
                }
                live_features[feature] = enhanced_df['ã‚¯ãƒ©ã‚¹_æ•°å€¤'].map(class_mapping).fillna(6)
            elif feature == 'æ–¤é‡':
                live_features[feature] = enhanced_df['æ–¤é‡_æ•°å€¤']
            elif feature == 'ä¸ŠãŒã‚Š':
                # ä¸ŠãŒã‚Šã‚¿ã‚¤ãƒ ã¯é€šå¸¸36.0å‰å¾Œãªã®ã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
                live_features[feature] = 36.0
            elif feature == 'é¨æ‰‹ã®å‹ç‡':
                # é¨æ‰‹ã®ä¸€èˆ¬çš„ãªå‹ç‡ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                live_features[feature] = 0.08
            elif feature == 'æ ç•ª':
                live_features[feature] = enhanced_df['æ '].astype(int)
            elif feature == 'å†…æ ':
                live_features[feature] = (enhanced_df['æ '].astype(int) <= 3).astype(int)
            elif feature == 'å¤–æ ':
                live_features[feature] = (enhanced_df['æ '].astype(int) >= 7).astype(int)
            elif feature == 'ä¸­æ ':
                live_features[feature] = ((enhanced_df['æ '].astype(int) >= 4) & (enhanced_df['æ '].astype(int) <= 6)).astype(int)
            # è¨“ç·´æ™‚ã«å­¦ç¿’ã—ãŸå®Ÿéš›ã®é¨æ‰‹ãƒ»èª¿æ•™å¸«å®Ÿç¸¾ã‚’ä½¿ç”¨
            elif feature == 'é¨æ‰‹å®Ÿå‹ç‡':
                if self.jockey_stats and 'å®Ÿå‹ç‡' in self.jockey_stats:
                    live_features[feature] = enhanced_df['é¨æ‰‹'].map(self.jockey_stats['å®Ÿå‹ç‡']).fillna(0.08)
                else:
                    live_features[feature] = 0.08
            elif feature == 'é¨æ‰‹å®Ÿè¤‡å‹ç‡':
                if self.jockey_stats and 'å®Ÿè¤‡å‹ç‡' in self.jockey_stats:
                    live_features[feature] = enhanced_df['é¨æ‰‹'].map(self.jockey_stats['å®Ÿè¤‡å‹ç‡']).fillna(0.25)
                else:
                    live_features[feature] = 0.25
            elif feature == 'é¨æ‰‹é¨ä¹—æ•°':
                if self.jockey_stats and 'é¨ä¹—æ•°' in self.jockey_stats:
                    live_features[feature] = enhanced_df['é¨æ‰‹'].map(self.jockey_stats['é¨ä¹—æ•°']).fillna(50)
                else:
                    live_features[feature] = 50
            elif feature == 'é¨æ‰‹å¹³å‡ä¸ŠãŒã‚Š':
                if self.jockey_stats and 'å¹³å‡ä¸ŠãŒã‚Š' in self.jockey_stats:
                    live_features[feature] = enhanced_df['é¨æ‰‹'].map(self.jockey_stats['å¹³å‡ä¸ŠãŒã‚Š']).fillna(35.0)
                else:
                    live_features[feature] = 35.0
            elif feature == 'èª¿æ•™å¸«å®Ÿå‹ç‡':
                if self.trainer_stats and 'å®Ÿå‹ç‡' in self.trainer_stats:
                    live_features[feature] = enhanced_df['èª¿æ•™å¸«'].map(self.trainer_stats['å®Ÿå‹ç‡']).fillna(0.06)
                else:
                    live_features[feature] = 0.06
            elif feature == 'èª¿æ•™å¸«å®Ÿè¤‡å‹ç‡':
                if self.trainer_stats and 'å®Ÿè¤‡å‹ç‡' in self.trainer_stats:
                    live_features[feature] = enhanced_df['èª¿æ•™å¸«'].map(self.trainer_stats['å®Ÿè¤‡å‹ç‡']).fillna(0.20)
                else:
                    live_features[feature] = 0.20
            elif feature == 'èª¿æ•™å¸«ç®¡ç†æ•°':
                if self.trainer_stats and 'ç®¡ç†æ•°' in self.trainer_stats:
                    live_features[feature] = enhanced_df['èª¿æ•™å¸«'].map(self.trainer_stats['ç®¡ç†æ•°']).fillna(100)
                else:
                    live_features[feature] = 100
            elif feature == 'èª¿æ•™å¸«å¹³å‡ä½“é‡':
                if self.trainer_stats and 'å¹³å‡ä½“é‡' in self.trainer_stats:
                    live_features[feature] = enhanced_df['èª¿æ•™å¸«'].map(self.trainer_stats['å¹³å‡ä½“é‡']).fillna(480)
                else:
                    live_features[feature] = 480
            elif feature == 'æ€§':
                # æ€§åˆ¥ã‚’æ•°å€¤åŒ–ï¼ˆç‰¡=1, ç‰=2, ã‚»=3ï¼‰
                if 'æ€§é½¢' in enhanced_df.columns:
                    sex_map = {'ç‰¡': 1, 'ç‰': 2, 'ã‚»': 3}
                    enhanced_df['æ€§_æ•°å€¤'] = enhanced_df['æ€§é½¢'].str[0].map(sex_map).fillna(1)
                    live_features[feature] = enhanced_df['æ€§_æ•°å€¤']
                else:
                    live_features[feature] = 1
            elif feature == 'é½¢':
                # å¹´é½¢ã‚’æ•°å€¤åŒ–
                if 'æ€§é½¢' in enhanced_df.columns:
                    enhanced_df['é½¢_æ•°å€¤'] = enhanced_df['æ€§é½¢'].str[1:].astype(int)
                    live_features[feature] = enhanced_df['é½¢_æ•°å€¤']
                else:
                    live_features[feature] = 4
            elif feature == 'èŠãƒ»ãƒ€ãƒ¼ãƒˆ':
                # èŠ=1, ãƒ€ãƒ¼ãƒˆ=2
                surface_map = {'èŠ': 1, 'ãƒ€ãƒ¼ãƒˆ': 2}
                live_features[feature] = enhanced_df['èŠãƒ€ãƒ¼ãƒˆ_æ•°å€¤'].map(surface_map).fillna(1)
            else:
                # ãã®ä»–ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆçµ±è¨ˆçš„ã«å¦¥å½“ãªå€¤ï¼‰
                if 'ç€é †' in feature:
                    live_features[feature] = 5.5
                elif 'å‹ç‡' in feature:
                    live_features[feature] = 0.08
                elif 'è·é›¢' in feature:
                    live_features[feature] = 2000
                elif 'é€šé' in feature:
                    live_features[feature] = 8.0
                elif 'ã‚¿ã‚¤ãƒ ' in feature:
                    live_features[feature] = 120.0
                elif 'ä½“é‡' in feature:
                    live_features[feature] = 480.0
                elif 'èƒ½åŠ›' in feature:
                    live_features[feature] = 0.5
                elif 'é©æ­£' in feature:
                    live_features[feature] = 1.0
                else:
                    live_features[feature] = 0.5
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        live_features = live_features.fillna(0)
        live_features = live_features.replace([np.inf, -np.inf], 0)
        
        return live_features
    
    def run_clean_system(self, race_data_file: str):
        """ã‚¯ãƒªãƒ¼ãƒ³ãªã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ"""
        print("ğŸš€ ã‚¯ãƒªãƒ¼ãƒ³ãª19ä¸‡ä»¶ãƒ‡ãƒ¼ã‚¿ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œé–‹å§‹")
        print("ğŸ’¡ ã‚ªãƒƒã‚ºã‚’ä½¿ã‚ãªã„çœŸã®æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹äºˆæ¸¬")
        
        # 1. å…¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        df = self.load_full_training_data()
        if df is None:
            return None
        
        # 2. ã‚¯ãƒªãƒ¼ãƒ³ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
        X, y = self.create_clean_features(df)
        if X is None:
            return None
        
        # 3. ã‚¯ãƒªãƒ¼ãƒ³ãªãƒ¢ãƒ‡ãƒ«è¨“ç·´
        model = self.train_clean_model(X, y)
        if model is None:
            return None
        
        # 4. ãƒ©ã‚¤ãƒ–äºˆæ¸¬
        print(f"\nğŸ‡ ã‚¯ãƒªãƒ¼ãƒ³ãªãƒ©ã‚¤ãƒ–äºˆæ¸¬å®Ÿè¡Œ")
        race_data = pd.read_csv(race_data_file)
        print(f"ğŸ“Š ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿: {len(race_data)}é ­")
        
        results = self.predict_with_clean_ml(race_data)
        if results is None:
            return None
        
        # 5. çµæœè¡¨ç¤º
        print("\nğŸ¯ ã‚¯ãƒªãƒ¼ãƒ³MLäºˆæ¸¬çµæœï¼ˆã‚ªãƒƒã‚ºéä½¿ç”¨ï¼‰:")
        print("="*130)
        print(f"{'é †ä½':>2} {'é¦¬ç•ª':>3} {'é¦¬å':>12} {'ã‚ªãƒƒã‚º':>6} {'ã‚¯ãƒªãƒ¼ãƒ³MLå‹ç‡':>11} {'ã‚¯ãƒªãƒ¼ãƒ³MLæœŸå¾…å€¤':>13} {'MLç€é †':>6}")
        print("="*110)
        
        for i, (_, horse) in enumerate(results.head(10).iterrows()):
            print(f"{i+1:2d}. {horse['é¦¬ç•ª']:2d}ç•ª {horse['é¦¬å']:12s} "
                  f"{horse['å˜å‹ã‚ªãƒƒã‚º']:5.1f}å€ {horse['ã‚¯ãƒªãƒ¼ãƒ³MLå‹åˆ©ç¢ºç‡']*100:8.1f}% "
                  f"{horse['ã‚¯ãƒªãƒ¼ãƒ³MLæœŸå¾…å€¤']:10.2f} {horse['ã‚¯ãƒªãƒ¼ãƒ³MLäºˆæ¸¬ç€é †']:5d}ç€")
        
        # ç€é †äºˆæ¸¬
        print(f"\nğŸ† ã‚¯ãƒªãƒ¼ãƒ³MLç€é †äºˆæ¸¬:")
        print("="*90)
        predicted_order = results.sort_values('ã‚¯ãƒªãƒ¼ãƒ³MLäºˆæ¸¬ç€é †')
        for _, horse in predicted_order.head(8).iterrows():
            print(f"{horse['ã‚¯ãƒªãƒ¼ãƒ³MLäºˆæ¸¬ç€é †']:2d}ç€äºˆæƒ³: {horse['é¦¬ç•ª']:2d}ç•ª {horse['é¦¬å']:12s} "
                  f"(ã‚¯ãƒªãƒ¼ãƒ³MLå‹ç‡{horse['ã‚¯ãƒªãƒ¼ãƒ³MLå‹åˆ©ç¢ºç‡']*100:5.1f}% æœŸå¾…å€¤{horse['ã‚¯ãƒªãƒ¼ãƒ³MLæœŸå¾…å€¤']:5.2f})")
        
        # æŠ•è³‡æ¨å¥¨
        print(f"\nğŸ’° ã‚¯ãƒªãƒ¼ãƒ³MLæŠ•è³‡æ¨å¥¨:")
        print("="*80)
        
        profitable = results[results['ã‚¯ãƒªãƒ¼ãƒ³MLæœŸå¾…å€¤'] >= 1.0]
        
        if len(profitable) > 0:
            print(f"ã€æœŸå¾…å€¤1.0ä»¥ä¸Šã€‘ {len(profitable)}é ­")
            for _, horse in profitable.head(3).iterrows():
                confidence = "è¶…é«˜" if horse['ã‚¯ãƒªãƒ¼ãƒ³MLæœŸå¾…å€¤'] >= 1.4 else "é«˜" if horse['ã‚¯ãƒªãƒ¼ãƒ³MLæœŸå¾…å€¤'] >= 1.2 else "ä¸­"
                print(f"  {horse['é¦¬ç•ª']:2d}ç•ª {horse['é¦¬å']:12s} ã‚¯ãƒªãƒ¼ãƒ³MLæœŸå¾…å€¤{horse['ã‚¯ãƒªãƒ¼ãƒ³MLæœŸå¾…å€¤']:5.2f} "
                      f"äºˆæ¸¬{horse['ã‚¯ãƒªãƒ¼ãƒ³MLäºˆæ¸¬ç€é †']:2d}ç€ ä¿¡é ¼åº¦:{confidence}")
            
            best = profitable.iloc[0]
            print(f"\nğŸ’¡ ã‚¯ãƒªãƒ¼ãƒ³MLæœ€æ¨å¥¨: {best['é¦¬ç•ª']}ç•ª{best['é¦¬å']} (æœŸå¾…å€¤{best['ã‚¯ãƒªãƒ¼ãƒ³MLæœŸå¾…å€¤']:.2f})")
        else:
            print("âŒ æœŸå¾…å€¤1.0ä»¥ä¸Šã®é¦¬ãªã—")
            top_predicted = predicted_order.iloc[0]
            print(f"ğŸ’¡ ç€é †é‡è¦–æ¨å¥¨: {top_predicted['é¦¬ç•ª']}ç•ª{top_predicted['é¦¬å']} "
                  f"(1ç€äºˆæƒ³ã€ã‚¯ãƒªãƒ¼ãƒ³MLå‹ç‡{top_predicted['ã‚¯ãƒªãƒ¼ãƒ³MLå‹åˆ©ç¢ºç‡']*100:.1f}%)")
        
        # ã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½
        print(f"\nğŸ“Š ã‚¯ãƒªãƒ¼ãƒ³ã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½:")
        print(f"   AUC: {self.model_metrics['test_auc']:.3f}")
        print(f"   ç²¾åº¦: {self.model_metrics['test_accuracy']:.3f}")
        print(f"   OOBç²¾åº¦: {self.model_metrics['oob_score']:.3f}")
        print(f"   ç‰¹å¾´é‡æ•°: {self.model_metrics['feature_count']}å€‹")
        print(f"   è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«: {self.model_metrics['training_samples']:,}ä»¶")
        print(f"   ç·ãƒ‡ãƒ¼ã‚¿æ´»ç”¨: {self.model_metrics['total_data_used']:,}ä»¶")
        print(f"   âš¡ ã‚ªãƒƒã‚ºéä¾å­˜ã®çœŸã®æ©Ÿæ¢°å­¦ç¿’")
        
        print(f"\nâœ… ã‚¯ãƒªãƒ¼ãƒ³ãª19ä¸‡ä»¶ãƒ‡ãƒ¼ã‚¿MLã‚·ã‚¹ãƒ†ãƒ å®Œäº†")
        return results


def main():
    """å®Ÿè¡Œ"""
    system = CleanFullDataMLSystem()
    results = system.run_clean_system("live_race_data_202505021211.csv")


if __name__ == "__main__":
    main()